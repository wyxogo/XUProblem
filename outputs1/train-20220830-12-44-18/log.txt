2022-08-30 12:44:18,145 Now training epoch 1. LR=0.000100
2022-08-30 12:44:29,323 Epoch[001/050], Step[0000/0079], Avg Loss: 0.0681, Avg Acc: 0.0781
2022-08-30 12:44:31,441 Epoch[001/050], Step[0020/0079], Avg Loss: 0.0299, Avg Acc: 0.3757
2022-08-30 12:44:33,515 Epoch[001/050], Step[0040/0079], Avg Loss: 0.0231, Avg Acc: 0.5221
2022-08-30 12:44:35,590 Epoch[001/050], Step[0060/0079], Avg Loss: 0.0203, Avg Acc: 0.5815
2022-08-30 12:44:37,489 ----- Epoch[001/050], Train Loss: 0.0186, Train Acc: 0.6240, time: 15.29
2022-08-30 12:44:37,489 Now training epoch 2. LR=0.000100
2022-08-30 12:44:37,716 Epoch[002/050], Step[0000/0079], Avg Loss: 0.0089, Avg Acc: 0.8125
2022-08-30 12:44:39,801 Epoch[002/050], Step[0020/0079], Avg Loss: 0.0114, Avg Acc: 0.7790
2022-08-30 12:44:41,879 Epoch[002/050], Step[0040/0079], Avg Loss: 0.0104, Avg Acc: 0.7988
2022-08-30 12:44:43,964 Epoch[002/050], Step[0060/0079], Avg Loss: 0.0098, Avg Acc: 0.8102
2022-08-30 12:44:45,858 ----- Epoch[002/050], Train Loss: 0.0098, Train Acc: 0.8106, time: 8.37
2022-08-30 12:44:45,858 Now training epoch 3. LR=0.000100
2022-08-30 12:44:46,080 Epoch[003/050], Step[0000/0079], Avg Loss: 0.0084, Avg Acc: 0.8594
2022-08-30 12:44:48,168 Epoch[003/050], Step[0020/0079], Avg Loss: 0.0068, Avg Acc: 0.8616
2022-08-30 12:44:50,258 Epoch[003/050], Step[0040/0079], Avg Loss: 0.0075, Avg Acc: 0.8495
2022-08-30 12:44:52,344 Epoch[003/050], Step[0060/0079], Avg Loss: 0.0076, Avg Acc: 0.8514
2022-08-30 12:44:54,250 ----- Epoch[003/050], Train Loss: 0.0077, Train Acc: 0.8502, time: 8.39
2022-08-30 12:44:54,250 Now training epoch 4. LR=0.000100
2022-08-30 12:44:54,477 Epoch[004/050], Step[0000/0079], Avg Loss: 0.0083, Avg Acc: 0.8594
2022-08-30 12:44:56,578 Epoch[004/050], Step[0020/0079], Avg Loss: 0.0075, Avg Acc: 0.8452
2022-08-30 12:44:58,680 Epoch[004/050], Step[0040/0079], Avg Loss: 0.0072, Avg Acc: 0.8537
2022-08-30 12:45:00,785 Epoch[004/050], Step[0060/0079], Avg Loss: 0.0069, Avg Acc: 0.8589
2022-08-30 12:45:02,695 ----- Epoch[004/050], Train Loss: 0.0069, Train Acc: 0.8658, time: 8.44
2022-08-30 12:45:02,696 Now training epoch 5. LR=0.000100
2022-08-30 12:45:02,919 Epoch[005/050], Step[0000/0079], Avg Loss: 0.0042, Avg Acc: 0.9062
2022-08-30 12:45:05,032 Epoch[005/050], Step[0020/0079], Avg Loss: 0.0059, Avg Acc: 0.8869
2022-08-30 12:45:07,141 Epoch[005/050], Step[0040/0079], Avg Loss: 0.0059, Avg Acc: 0.8784
2022-08-30 12:45:09,253 Epoch[005/050], Step[0060/0079], Avg Loss: 0.0056, Avg Acc: 0.8842
2022-08-30 12:45:11,174 ----- Epoch[005/050], Train Loss: 0.0057, Train Acc: 0.8890, time: 8.48
2022-08-30 12:45:11,175 Now training epoch 6. LR=0.000100
2022-08-30 12:45:11,401 Epoch[006/050], Step[0000/0079], Avg Loss: 0.0086, Avg Acc: 0.8125
2022-08-30 12:45:13,515 Epoch[006/050], Step[0020/0079], Avg Loss: 0.0053, Avg Acc: 0.8936
2022-08-30 12:45:15,632 Epoch[006/050], Step[0040/0079], Avg Loss: 0.0053, Avg Acc: 0.9017
2022-08-30 12:45:17,758 Epoch[006/050], Step[0060/0079], Avg Loss: 0.0053, Avg Acc: 0.9024
2022-08-30 12:45:19,688 ----- Epoch[006/050], Train Loss: 0.0052, Train Acc: 0.9026, time: 8.51
2022-08-30 12:45:19,689 Now training epoch 7. LR=0.000100
2022-08-30 12:45:19,912 Epoch[007/050], Step[0000/0079], Avg Loss: 0.0028, Avg Acc: 0.9375
2022-08-30 12:45:22,033 Epoch[007/050], Step[0020/0079], Avg Loss: 0.0053, Avg Acc: 0.8951
2022-08-30 12:45:24,160 Epoch[007/050], Step[0040/0079], Avg Loss: 0.0051, Avg Acc: 0.9021
2022-08-30 12:45:26,296 Epoch[007/050], Step[0060/0079], Avg Loss: 0.0047, Avg Acc: 0.9088
2022-08-30 12:45:28,232 ----- Epoch[007/050], Train Loss: 0.0046, Train Acc: 0.9102, time: 8.54
2022-08-30 12:45:28,233 Now training epoch 8. LR=0.000100
2022-08-30 12:45:28,462 Epoch[008/050], Step[0000/0079], Avg Loss: 0.0070, Avg Acc: 0.8594
2022-08-30 12:45:30,600 Epoch[008/050], Step[0020/0079], Avg Loss: 0.0038, Avg Acc: 0.9249
2022-08-30 12:45:32,739 Epoch[008/050], Step[0040/0079], Avg Loss: 0.0037, Avg Acc: 0.9268
2022-08-30 12:45:34,873 Epoch[008/050], Step[0060/0079], Avg Loss: 0.0038, Avg Acc: 0.9270
2022-08-30 12:45:36,810 ----- Epoch[008/050], Train Loss: 0.0039, Train Acc: 0.9242, time: 8.58
2022-08-30 12:45:36,810 Now training epoch 9. LR=0.000100
2022-08-30 12:45:37,045 Epoch[009/050], Step[0000/0079], Avg Loss: 0.0026, Avg Acc: 0.9375
2022-08-30 12:45:39,194 Epoch[009/050], Step[0020/0079], Avg Loss: 0.0036, Avg Acc: 0.9241
2022-08-30 12:45:41,341 Epoch[009/050], Step[0040/0079], Avg Loss: 0.0036, Avg Acc: 0.9299
2022-08-30 12:45:43,481 Epoch[009/050], Step[0060/0079], Avg Loss: 0.0037, Avg Acc: 0.9290
2022-08-30 12:45:45,459 ----- Epoch[009/050], Train Loss: 0.0037, Train Acc: 0.9284, time: 8.65
2022-08-30 12:45:45,460 Now training epoch 10. LR=0.000100
2022-08-30 12:45:45,685 Epoch[010/050], Step[0000/0079], Avg Loss: 0.0046, Avg Acc: 0.8750
2022-08-30 12:45:47,839 Epoch[010/050], Step[0020/0079], Avg Loss: 0.0036, Avg Acc: 0.9368
2022-08-30 12:45:49,999 Epoch[010/050], Step[0040/0079], Avg Loss: 0.0035, Avg Acc: 0.9364
2022-08-30 12:45:52,153 Epoch[010/050], Step[0060/0079], Avg Loss: 0.0035, Avg Acc: 0.9349
2022-08-30 12:45:54,107 ----- Epoch[010/050], Train Loss: 0.0034, Train Acc: 0.9348, time: 8.65
2022-08-30 12:45:54,107 Now training epoch 11. LR=0.000100
2022-08-30 12:45:54,331 Epoch[011/050], Step[0000/0079], Avg Loss: 0.0039, Avg Acc: 0.9219
2022-08-30 12:45:56,487 Epoch[011/050], Step[0020/0079], Avg Loss: 0.0030, Avg Acc: 0.9405
2022-08-30 12:45:58,640 Epoch[011/050], Step[0040/0079], Avg Loss: 0.0037, Avg Acc: 0.9306
2022-08-30 12:46:00,796 Epoch[011/050], Step[0060/0079], Avg Loss: 0.0037, Avg Acc: 0.9314
2022-08-30 12:46:02,741 ----- Epoch[011/050], Train Loss: 0.0037, Train Acc: 0.9314, time: 8.63
2022-08-30 12:46:02,741 Now training epoch 12. LR=0.000100
2022-08-30 12:46:02,973 Epoch[012/050], Step[0000/0079], Avg Loss: 0.0014, Avg Acc: 0.9688
2022-08-30 12:46:05,119 Epoch[012/050], Step[0020/0079], Avg Loss: 0.0038, Avg Acc: 0.9263
2022-08-30 12:46:07,272 Epoch[012/050], Step[0040/0079], Avg Loss: 0.0031, Avg Acc: 0.9394
2022-08-30 12:46:09,418 Epoch[012/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9362
2022-08-30 12:46:11,361 ----- Epoch[012/050], Train Loss: 0.0032, Train Acc: 0.9398, time: 8.62
2022-08-30 12:46:11,361 Now training epoch 13. LR=0.000100
2022-08-30 12:46:11,606 Epoch[013/050], Step[0000/0079], Avg Loss: 0.0019, Avg Acc: 0.9688
2022-08-30 12:46:13,735 Epoch[013/050], Step[0020/0079], Avg Loss: 0.0026, Avg Acc: 0.9501
2022-08-30 12:46:15,870 Epoch[013/050], Step[0040/0079], Avg Loss: 0.0028, Avg Acc: 0.9455
2022-08-30 12:46:18,011 Epoch[013/050], Step[0060/0079], Avg Loss: 0.0027, Avg Acc: 0.9480
2022-08-30 12:46:19,958 ----- Epoch[013/050], Train Loss: 0.0027, Train Acc: 0.9480, time: 8.60
2022-08-30 12:46:19,958 Now training epoch 14. LR=0.000100
2022-08-30 12:46:20,175 Epoch[014/050], Step[0000/0079], Avg Loss: 0.0029, Avg Acc: 0.9688
2022-08-30 12:46:22,306 Epoch[014/050], Step[0020/0079], Avg Loss: 0.0017, Avg Acc: 0.9650
2022-08-30 12:46:24,447 Epoch[014/050], Step[0040/0079], Avg Loss: 0.0019, Avg Acc: 0.9600
2022-08-30 12:46:26,654 Epoch[014/050], Step[0060/0079], Avg Loss: 0.0023, Avg Acc: 0.9536
2022-08-30 12:46:28,594 ----- Epoch[014/050], Train Loss: 0.0025, Train Acc: 0.9514, time: 8.63
2022-08-30 12:46:28,594 Now training epoch 15. LR=0.000100
2022-08-30 12:46:28,828 Epoch[015/050], Step[0000/0079], Avg Loss: 0.0023, Avg Acc: 0.9375
2022-08-30 12:46:30,958 Epoch[015/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9256
2022-08-30 12:46:33,096 Epoch[015/050], Step[0040/0079], Avg Loss: 0.0038, Avg Acc: 0.9348
2022-08-30 12:46:35,230 Epoch[015/050], Step[0060/0079], Avg Loss: 0.0035, Avg Acc: 0.9413
2022-08-30 12:46:37,156 ----- Epoch[015/050], Train Loss: 0.0035, Train Acc: 0.9436, time: 8.56
2022-08-30 12:46:37,157 Now training epoch 16. LR=0.000100
2022-08-30 12:46:37,402 Epoch[016/050], Step[0000/0079], Avg Loss: 0.0012, Avg Acc: 0.9844
2022-08-30 12:46:39,537 Epoch[016/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9501
2022-08-30 12:46:41,667 Epoch[016/050], Step[0040/0079], Avg Loss: 0.0026, Avg Acc: 0.9554
2022-08-30 12:46:43,793 Epoch[016/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9547
2022-08-30 12:46:45,723 ----- Epoch[016/050], Train Loss: 0.0026, Train Acc: 0.9542, time: 8.57
2022-08-30 12:46:45,723 Now training epoch 17. LR=0.000100
2022-08-30 12:46:45,943 Epoch[017/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9375
2022-08-30 12:46:48,072 Epoch[017/050], Step[0020/0079], Avg Loss: 0.0025, Avg Acc: 0.9598
2022-08-30 12:46:50,200 Epoch[017/050], Step[0040/0079], Avg Loss: 0.0024, Avg Acc: 0.9566
2022-08-30 12:46:52,327 Epoch[017/050], Step[0060/0079], Avg Loss: 0.0025, Avg Acc: 0.9544
2022-08-30 12:46:54,254 ----- Epoch[017/050], Train Loss: 0.0026, Train Acc: 0.9564, time: 8.53
2022-08-30 12:46:54,254 Now training epoch 18. LR=0.000100
2022-08-30 12:46:54,480 Epoch[018/050], Step[0000/0079], Avg Loss: 0.0009, Avg Acc: 0.9844
2022-08-30 12:46:56,607 Epoch[018/050], Step[0020/0079], Avg Loss: 0.0019, Avg Acc: 0.9680
2022-08-30 12:46:58,749 Epoch[018/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9592
2022-08-30 12:47:00,881 Epoch[018/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9639
2022-08-30 12:47:02,816 ----- Epoch[018/050], Train Loss: 0.0023, Train Acc: 0.9620, time: 8.56
2022-08-30 12:47:02,817 Now training epoch 19. LR=0.000100
2022-08-30 12:47:03,041 Epoch[019/050], Step[0000/0079], Avg Loss: 0.0019, Avg Acc: 0.9688
2022-08-30 12:47:05,183 Epoch[019/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9613
2022-08-30 12:47:07,318 Epoch[019/050], Step[0040/0079], Avg Loss: 0.0021, Avg Acc: 0.9646
2022-08-30 12:47:09,451 Epoch[019/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9634
2022-08-30 12:47:11,382 ----- Epoch[019/050], Train Loss: 0.0020, Train Acc: 0.9654, time: 8.56
2022-08-30 12:47:11,382 Now training epoch 20. LR=0.000100
2022-08-30 12:47:11,627 Epoch[020/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9375
2022-08-30 12:47:13,761 Epoch[020/050], Step[0020/0079], Avg Loss: 0.0020, Avg Acc: 0.9583
2022-08-30 12:47:15,893 Epoch[020/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9573
2022-08-30 12:47:18,053 Epoch[020/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9606
2022-08-30 12:47:19,991 ----- Epoch[020/050], Train Loss: 0.0019, Train Acc: 0.9640, time: 8.61
2022-08-30 12:47:19,991 Now training epoch 21. LR=0.000100
2022-08-30 12:47:20,211 Epoch[021/050], Step[0000/0079], Avg Loss: 0.0020, Avg Acc: 0.9531
2022-08-30 12:47:22,353 Epoch[021/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9546
2022-08-30 12:47:24,498 Epoch[021/050], Step[0040/0079], Avg Loss: 0.0023, Avg Acc: 0.9585
2022-08-30 12:47:26,636 Epoch[021/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9613
2022-08-30 12:47:28,575 ----- Epoch[021/050], Train Loss: 0.0020, Train Acc: 0.9644, time: 8.58
2022-08-30 12:47:28,576 Now training epoch 22. LR=0.000100
2022-08-30 12:47:28,803 Epoch[022/050], Step[0000/0079], Avg Loss: 0.0002, Avg Acc: 1.0000
2022-08-30 12:47:30,938 Epoch[022/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9807
2022-08-30 12:47:33,074 Epoch[022/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9775
2022-08-30 12:47:35,211 Epoch[022/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9769
2022-08-30 12:47:37,143 ----- Epoch[022/050], Train Loss: 0.0015, Train Acc: 0.9718, time: 8.57
2022-08-30 12:47:37,143 Now training epoch 23. LR=0.000100
2022-08-30 12:47:37,367 Epoch[023/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9531
2022-08-30 12:47:39,500 Epoch[023/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9591
2022-08-30 12:47:41,633 Epoch[023/050], Step[0040/0079], Avg Loss: 0.0019, Avg Acc: 0.9668
2022-08-30 12:47:43,764 Epoch[023/050], Step[0060/0079], Avg Loss: 0.0019, Avg Acc: 0.9662
2022-08-30 12:47:45,699 ----- Epoch[023/050], Train Loss: 0.0020, Train Acc: 0.9676, time: 8.55
2022-08-30 12:47:45,700 Now training epoch 24. LR=0.000100
2022-08-30 12:47:45,920 Epoch[024/050], Step[0000/0079], Avg Loss: 0.0034, Avg Acc: 0.9375
2022-08-30 12:47:48,055 Epoch[024/050], Step[0020/0079], Avg Loss: 0.0019, Avg Acc: 0.9665
2022-08-30 12:47:50,189 Epoch[024/050], Step[0040/0079], Avg Loss: 0.0017, Avg Acc: 0.9699
2022-08-30 12:47:52,318 Epoch[024/050], Step[0060/0079], Avg Loss: 0.0017, Avg Acc: 0.9698
2022-08-30 12:47:54,248 ----- Epoch[024/050], Train Loss: 0.0017, Train Acc: 0.9694, time: 8.55
2022-08-30 12:47:54,248 Now training epoch 25. LR=0.000100
2022-08-30 12:47:54,484 Epoch[025/050], Step[0000/0079], Avg Loss: 0.0005, Avg Acc: 1.0000
2022-08-30 12:47:56,612 Epoch[025/050], Step[0020/0079], Avg Loss: 0.0013, Avg Acc: 0.9732
2022-08-30 12:47:58,742 Epoch[025/050], Step[0040/0079], Avg Loss: 0.0017, Avg Acc: 0.9684
2022-08-30 12:48:00,880 Epoch[025/050], Step[0060/0079], Avg Loss: 0.0017, Avg Acc: 0.9682
2022-08-30 12:48:02,815 ----- Epoch[025/050], Train Loss: 0.0019, Train Acc: 0.9686, time: 8.57
2022-08-30 12:48:07,385 ----- Save model: ./outputs1/train-20220830-12-44-18/Epoch-25-Loss-0.0018840523378923536.pth
2022-08-30 12:48:07,385 Now training epoch 26. LR=0.000100
2022-08-30 12:48:07,667 Epoch[026/050], Step[0000/0079], Avg Loss: 0.0013, Avg Acc: 0.9531
2022-08-30 12:48:09,829 Epoch[026/050], Step[0020/0079], Avg Loss: 0.0031, Avg Acc: 0.9360
2022-08-30 12:48:11,955 Epoch[026/050], Step[0040/0079], Avg Loss: 0.0031, Avg Acc: 0.9386
2022-08-30 12:48:14,087 Epoch[026/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9475
2022-08-30 12:48:16,026 ----- Epoch[026/050], Train Loss: 0.0023, Train Acc: 0.9530, time: 8.64
2022-08-30 12:48:16,026 Now training epoch 27. LR=0.000100
2022-08-30 12:48:16,261 Epoch[027/050], Step[0000/0079], Avg Loss: 0.0002, Avg Acc: 1.0000
2022-08-30 12:48:18,396 Epoch[027/050], Step[0020/0079], Avg Loss: 0.0012, Avg Acc: 0.9792
2022-08-30 12:48:20,524 Epoch[027/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9806
2022-08-30 12:48:22,672 Epoch[027/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9795
2022-08-30 12:48:24,623 ----- Epoch[027/050], Train Loss: 0.0012, Train Acc: 0.9762, time: 8.60
2022-08-30 12:48:24,623 Now training epoch 28. LR=0.000100
2022-08-30 12:48:24,853 Epoch[028/050], Step[0000/0079], Avg Loss: 0.0013, Avg Acc: 0.9688
2022-08-30 12:48:26,992 Epoch[028/050], Step[0020/0079], Avg Loss: 0.0014, Avg Acc: 0.9702
2022-08-30 12:48:29,124 Epoch[028/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9748
2022-08-30 12:48:31,258 Epoch[028/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9775
2022-08-30 12:48:33,208 ----- Epoch[028/050], Train Loss: 0.0012, Train Acc: 0.9754, time: 8.58
2022-08-30 12:48:33,208 Now training epoch 29. LR=0.000100
2022-08-30 12:48:33,443 Epoch[029/050], Step[0000/0079], Avg Loss: 0.0025, Avg Acc: 0.9375
2022-08-30 12:48:35,586 Epoch[029/050], Step[0020/0079], Avg Loss: 0.0016, Avg Acc: 0.9695
2022-08-30 12:48:37,726 Epoch[029/050], Step[0040/0079], Avg Loss: 0.0014, Avg Acc: 0.9745
2022-08-30 12:48:39,877 Epoch[029/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9728
2022-08-30 12:48:41,932 ----- Epoch[029/050], Train Loss: 0.0016, Train Acc: 0.9722, time: 8.72
2022-08-30 12:48:41,932 Now training epoch 30. LR=0.000100
2022-08-30 12:48:42,152 Epoch[030/050], Step[0000/0079], Avg Loss: 0.0009, Avg Acc: 0.9531
2022-08-30 12:48:44,293 Epoch[030/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9799
2022-08-30 12:48:46,428 Epoch[030/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9779
2022-08-30 12:48:48,565 Epoch[030/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9739
2022-08-30 12:48:50,527 ----- Epoch[030/050], Train Loss: 0.0015, Train Acc: 0.9726, time: 8.59
2022-08-30 12:48:50,527 Now training epoch 31. LR=0.000100
2022-08-30 12:48:50,756 Epoch[031/050], Step[0000/0079], Avg Loss: 0.0018, Avg Acc: 0.9844
2022-08-30 12:48:52,899 Epoch[031/050], Step[0020/0079], Avg Loss: 0.0013, Avg Acc: 0.9784
2022-08-30 12:48:55,036 Epoch[031/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9832
2022-08-30 12:48:57,179 Epoch[031/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9816
2022-08-30 12:48:59,125 ----- Epoch[031/050], Train Loss: 0.0012, Train Acc: 0.9800, time: 8.60
2022-08-30 12:48:59,125 Now training epoch 32. LR=0.000100
2022-08-30 12:48:59,360 Epoch[032/050], Step[0000/0079], Avg Loss: 0.0035, Avg Acc: 0.9375
2022-08-30 12:49:01,504 Epoch[032/050], Step[0020/0079], Avg Loss: 0.0015, Avg Acc: 0.9769
2022-08-30 12:49:03,642 Epoch[032/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9809
2022-08-30 12:49:05,777 Epoch[032/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9808
2022-08-30 12:49:07,723 ----- Epoch[032/050], Train Loss: 0.0013, Train Acc: 0.9796, time: 8.60
2022-08-30 12:49:07,724 Now training epoch 33. LR=0.000100
2022-08-30 12:49:07,965 Epoch[033/050], Step[0000/0079], Avg Loss: 0.0018, Avg Acc: 0.9844
2022-08-30 12:49:10,100 Epoch[033/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9821
2022-08-30 12:49:12,234 Epoch[033/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9783
2022-08-30 12:49:14,367 Epoch[033/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9746
2022-08-30 12:49:16,318 ----- Epoch[033/050], Train Loss: 0.0016, Train Acc: 0.9724, time: 8.59
2022-08-30 12:49:16,319 Now training epoch 34. LR=0.000100
2022-08-30 12:49:16,543 Epoch[034/050], Step[0000/0079], Avg Loss: 0.0022, Avg Acc: 0.9688
2022-08-30 12:49:18,675 Epoch[034/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9792
2022-08-30 12:49:20,803 Epoch[034/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9809
2022-08-30 12:49:22,931 Epoch[034/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9805
2022-08-30 12:49:24,871 ----- Epoch[034/050], Train Loss: 0.0012, Train Acc: 0.9820, time: 8.55
2022-08-30 12:49:24,871 Now training epoch 35. LR=0.000100
2022-08-30 12:49:25,097 Epoch[035/050], Step[0000/0079], Avg Loss: 0.0007, Avg Acc: 0.9844
2022-08-30 12:49:27,230 Epoch[035/050], Step[0020/0079], Avg Loss: 0.0014, Avg Acc: 0.9777
2022-08-30 12:49:29,358 Epoch[035/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9775
2022-08-30 12:49:31,500 Epoch[035/050], Step[0060/0079], Avg Loss: 0.0015, Avg Acc: 0.9739
2022-08-30 12:49:33,444 ----- Epoch[035/050], Train Loss: 0.0016, Train Acc: 0.9714, time: 8.57
2022-08-30 12:49:33,445 Now training epoch 36. LR=0.000100
2022-08-30 12:49:33,686 Epoch[036/050], Step[0000/0079], Avg Loss: 0.0016, Avg Acc: 0.9688
2022-08-30 12:49:35,822 Epoch[036/050], Step[0020/0079], Avg Loss: 0.0008, Avg Acc: 0.9844
2022-08-30 12:49:37,955 Epoch[036/050], Step[0040/0079], Avg Loss: 0.0008, Avg Acc: 0.9829
2022-08-30 12:49:40,087 Epoch[036/050], Step[0060/0079], Avg Loss: 0.0009, Avg Acc: 0.9813
2022-08-30 12:49:42,034 ----- Epoch[036/050], Train Loss: 0.0009, Train Acc: 0.9818, time: 8.59
2022-08-30 12:49:42,035 Now training epoch 37. LR=0.000100
2022-08-30 12:49:42,273 Epoch[037/050], Step[0000/0079], Avg Loss: 0.0007, Avg Acc: 0.9688
2022-08-30 12:49:44,407 Epoch[037/050], Step[0020/0079], Avg Loss: 0.0015, Avg Acc: 0.9784
2022-08-30 12:49:46,547 Epoch[037/050], Step[0040/0079], Avg Loss: 0.0016, Avg Acc: 0.9733
2022-08-30 12:49:48,683 Epoch[037/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9759
2022-08-30 12:49:50,627 ----- Epoch[037/050], Train Loss: 0.0012, Train Acc: 0.9782, time: 8.59
2022-08-30 12:49:50,628 Now training epoch 38. LR=0.000100
2022-08-30 12:49:50,876 Epoch[038/050], Step[0000/0079], Avg Loss: 0.0000, Avg Acc: 1.0000
2022-08-30 12:49:53,013 Epoch[038/050], Step[0020/0079], Avg Loss: 0.0007, Avg Acc: 0.9881
2022-08-30 12:49:55,143 Epoch[038/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9809
2022-08-30 12:49:57,271 Epoch[038/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9821
2022-08-30 12:49:59,211 ----- Epoch[038/050], Train Loss: 0.0011, Train Acc: 0.9818, time: 8.58
2022-08-30 12:49:59,211 Now training epoch 39. LR=0.000100
2022-08-30 12:49:59,446 Epoch[039/050], Step[0000/0079], Avg Loss: 0.0002, Avg Acc: 1.0000
2022-08-30 12:50:01,579 Epoch[039/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9777
2022-08-30 12:50:03,710 Epoch[039/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9798
2022-08-30 12:50:05,849 Epoch[039/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9798
2022-08-30 12:50:07,804 ----- Epoch[039/050], Train Loss: 0.0012, Train Acc: 0.9790, time: 8.59
2022-08-30 12:50:07,805 Now training epoch 40. LR=0.000100
2022-08-30 12:50:08,055 Epoch[040/050], Step[0000/0079], Avg Loss: 0.0030, Avg Acc: 0.9531
2022-08-30 12:50:10,186 Epoch[040/050], Step[0020/0079], Avg Loss: 0.0009, Avg Acc: 0.9866
2022-08-30 12:50:12,324 Epoch[040/050], Step[0040/0079], Avg Loss: 0.0009, Avg Acc: 0.9886
2022-08-30 12:50:14,459 Epoch[040/050], Step[0060/0079], Avg Loss: 0.0009, Avg Acc: 0.9874
2022-08-30 12:50:16,402 ----- Epoch[040/050], Train Loss: 0.0008, Train Acc: 0.9882, time: 8.60
2022-08-30 12:50:16,403 Now training epoch 41. LR=0.000100
2022-08-30 12:50:16,641 Epoch[041/050], Step[0000/0079], Avg Loss: 0.0006, Avg Acc: 0.9844
2022-08-30 12:50:18,778 Epoch[041/050], Step[0020/0079], Avg Loss: 0.0007, Avg Acc: 0.9881
2022-08-30 12:50:20,911 Epoch[041/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9817
2022-08-30 12:50:23,046 Epoch[041/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9831
2022-08-30 12:50:24,987 ----- Epoch[041/050], Train Loss: 0.0011, Train Acc: 0.9834, time: 8.58
2022-08-30 12:50:24,988 Now training epoch 42. LR=0.000100
2022-08-30 12:50:25,217 Epoch[042/050], Step[0000/0079], Avg Loss: 0.0001, Avg Acc: 1.0000
2022-08-30 12:50:27,351 Epoch[042/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9621
2022-08-30 12:50:29,481 Epoch[042/050], Step[0040/0079], Avg Loss: 0.0023, Avg Acc: 0.9653
2022-08-30 12:50:31,610 Epoch[042/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9688
2022-08-30 12:50:33,555 ----- Epoch[042/050], Train Loss: 0.0020, Train Acc: 0.9698, time: 8.57
2022-08-30 12:50:33,555 Now training epoch 43. LR=0.000100
2022-08-30 12:50:33,798 Epoch[043/050], Step[0000/0079], Avg Loss: 0.0008, Avg Acc: 0.9844
2022-08-30 12:50:35,932 Epoch[043/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9836
2022-08-30 12:50:38,067 Epoch[043/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9813
2022-08-30 12:50:40,193 Epoch[043/050], Step[0060/0079], Avg Loss: 0.0010, Avg Acc: 0.9821
2022-08-30 12:50:42,131 ----- Epoch[043/050], Train Loss: 0.0012, Train Acc: 0.9806, time: 8.57
2022-08-30 12:50:42,131 Now training epoch 44. LR=0.000100
2022-08-30 12:50:42,366 Epoch[044/050], Step[0000/0079], Avg Loss: 0.0020, Avg Acc: 0.9688
2022-08-30 12:50:44,500 Epoch[044/050], Step[0020/0079], Avg Loss: 0.0015, Avg Acc: 0.9717
2022-08-30 12:50:46,628 Epoch[044/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9752
2022-08-30 12:50:48,759 Epoch[044/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9759
2022-08-30 12:50:50,713 ----- Epoch[044/050], Train Loss: 0.0013, Train Acc: 0.9770, time: 8.58
2022-08-30 12:50:50,713 Now training epoch 45. LR=0.000100
2022-08-30 12:50:50,952 Epoch[045/050], Step[0000/0079], Avg Loss: 0.0005, Avg Acc: 1.0000
2022-08-30 12:50:53,091 Epoch[045/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9792
2022-08-30 12:50:55,325 Epoch[045/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9783
2022-08-30 12:50:57,463 Epoch[045/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9816
2022-08-30 12:50:59,408 ----- Epoch[045/050], Train Loss: 0.0011, Train Acc: 0.9816, time: 8.69
2022-08-30 12:50:59,408 Now training epoch 46. LR=0.000100
2022-08-30 12:50:59,637 Epoch[046/050], Step[0000/0079], Avg Loss: 0.0001, Avg Acc: 1.0000
2022-08-30 12:51:01,768 Epoch[046/050], Step[0020/0079], Avg Loss: 0.0016, Avg Acc: 0.9799
2022-08-30 12:51:03,901 Epoch[046/050], Step[0040/0079], Avg Loss: 0.0014, Avg Acc: 0.9790
2022-08-30 12:51:06,037 Epoch[046/050], Step[0060/0079], Avg Loss: 0.0015, Avg Acc: 0.9798
2022-08-30 12:51:07,986 ----- Epoch[046/050], Train Loss: 0.0013, Train Acc: 0.9804, time: 8.58
2022-08-30 12:51:07,986 Now training epoch 47. LR=0.000100
2022-08-30 12:51:08,216 Epoch[047/050], Step[0000/0079], Avg Loss: 0.0026, Avg Acc: 0.9844
2022-08-30 12:51:10,356 Epoch[047/050], Step[0020/0079], Avg Loss: 0.0012, Avg Acc: 0.9814
2022-08-30 12:51:12,489 Epoch[047/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9806
2022-08-30 12:51:14,623 Epoch[047/050], Step[0060/0079], Avg Loss: 0.0010, Avg Acc: 0.9826
2022-08-30 12:51:16,566 ----- Epoch[047/050], Train Loss: 0.0010, Train Acc: 0.9832, time: 8.58
2022-08-30 12:51:16,566 Now training epoch 48. LR=0.000100
2022-08-30 12:51:16,792 Epoch[048/050], Step[0000/0079], Avg Loss: 0.0033, Avg Acc: 0.9375
2022-08-30 12:51:18,936 Epoch[048/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9688
2022-08-30 12:51:21,065 Epoch[048/050], Step[0040/0079], Avg Loss: 0.0016, Avg Acc: 0.9771
2022-08-30 12:51:23,208 Epoch[048/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9787
2022-08-30 12:51:25,149 ----- Epoch[048/050], Train Loss: 0.0015, Train Acc: 0.9782, time: 8.58
2022-08-30 12:51:25,150 Now training epoch 49. LR=0.000100
2022-08-30 12:51:25,381 Epoch[049/050], Step[0000/0079], Avg Loss: 0.0003, Avg Acc: 1.0000
2022-08-30 12:51:27,510 Epoch[049/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9836
2022-08-30 12:51:29,638 Epoch[049/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9806
2022-08-30 12:51:31,769 Epoch[049/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9795
2022-08-30 12:51:33,710 ----- Epoch[049/050], Train Loss: 0.0011, Train Acc: 0.9796, time: 8.56
2022-08-30 12:51:33,710 Now training epoch 50. LR=0.000100
2022-08-30 12:51:33,942 Epoch[050/050], Step[0000/0079], Avg Loss: 0.0041, Avg Acc: 0.9375
2022-08-30 12:51:36,074 Epoch[050/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9814
2022-08-30 12:51:38,209 Epoch[050/050], Step[0040/0079], Avg Loss: 0.0009, Avg Acc: 0.9863
2022-08-30 12:51:40,348 Epoch[050/050], Step[0060/0079], Avg Loss: 0.0007, Avg Acc: 0.9892
2022-08-30 12:51:42,289 ----- Epoch[050/050], Train Loss: 0.0007, Train Acc: 0.9896, time: 8.58
2022-08-30 12:51:46,646 ----- Save model: ./outputs1/train-20220830-12-44-18/Epoch-50-Loss-0.000734963038371643.pth
