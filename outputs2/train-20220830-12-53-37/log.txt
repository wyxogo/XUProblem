2022-08-30 12:53:37,572 Now training epoch 1. LR=0.000100
2022-08-30 12:53:41,563 Epoch[001/050], Step[0000/0079], Avg Loss: 0.0656, Avg Acc: 0.0469
2022-08-30 12:53:43,672 Epoch[001/050], Step[0020/0079], Avg Loss: 0.0326, Avg Acc: 0.2746
2022-08-30 12:53:45,751 Epoch[001/050], Step[0040/0079], Avg Loss: 0.0274, Avg Acc: 0.3685
2022-08-30 12:53:47,844 Epoch[001/050], Step[0060/0079], Avg Loss: 0.0245, Avg Acc: 0.4370
2022-08-30 12:53:49,760 ----- Epoch[001/050], Train Loss: 0.0231, Train Acc: 0.4680, time: 10.42
2022-08-30 12:53:49,760 Now training epoch 2. LR=0.000100
2022-08-30 12:53:49,989 Epoch[002/050], Step[0000/0079], Avg Loss: 0.0168, Avg Acc: 0.6719
2022-08-30 12:53:52,073 Epoch[002/050], Step[0020/0079], Avg Loss: 0.0159, Avg Acc: 0.6257
2022-08-30 12:53:54,165 Epoch[002/050], Step[0040/0079], Avg Loss: 0.0156, Avg Acc: 0.6345
2022-08-30 12:53:56,265 Epoch[002/050], Step[0060/0079], Avg Loss: 0.0159, Avg Acc: 0.6296
2022-08-30 12:53:58,164 ----- Epoch[002/050], Train Loss: 0.0157, Train Acc: 0.6362, time: 8.40
2022-08-30 12:53:58,165 Now training epoch 3. LR=0.000100
2022-08-30 12:53:58,393 Epoch[003/050], Step[0000/0079], Avg Loss: 0.0150, Avg Acc: 0.6406
2022-08-30 12:54:00,493 Epoch[003/050], Step[0020/0079], Avg Loss: 0.0137, Avg Acc: 0.6749
2022-08-30 12:54:02,600 Epoch[003/050], Step[0040/0079], Avg Loss: 0.0140, Avg Acc: 0.6715
2022-08-30 12:54:04,710 Epoch[003/050], Step[0060/0079], Avg Loss: 0.0139, Avg Acc: 0.6770
2022-08-30 12:54:06,632 ----- Epoch[003/050], Train Loss: 0.0138, Train Acc: 0.6838, time: 8.47
2022-08-30 12:54:06,632 Now training epoch 4. LR=0.000100
2022-08-30 12:54:06,858 Epoch[004/050], Step[0000/0079], Avg Loss: 0.0095, Avg Acc: 0.7656
2022-08-30 12:54:08,974 Epoch[004/050], Step[0020/0079], Avg Loss: 0.0121, Avg Acc: 0.7016
2022-08-30 12:54:11,080 Epoch[004/050], Step[0040/0079], Avg Loss: 0.0120, Avg Acc: 0.7077
2022-08-30 12:54:13,188 Epoch[004/050], Step[0060/0079], Avg Loss: 0.0120, Avg Acc: 0.7190
2022-08-30 12:54:15,104 ----- Epoch[004/050], Train Loss: 0.0121, Train Acc: 0.7154, time: 8.47
2022-08-30 12:54:15,104 Now training epoch 5. LR=0.000100
2022-08-30 12:54:15,338 Epoch[005/050], Step[0000/0079], Avg Loss: 0.0062, Avg Acc: 0.8594
2022-08-30 12:54:17,453 Epoch[005/050], Step[0020/0079], Avg Loss: 0.0119, Avg Acc: 0.7269
2022-08-30 12:54:19,568 Epoch[005/050], Step[0040/0079], Avg Loss: 0.0114, Avg Acc: 0.7336
2022-08-30 12:54:21,705 Epoch[005/050], Step[0060/0079], Avg Loss: 0.0114, Avg Acc: 0.7362
2022-08-30 12:54:23,648 ----- Epoch[005/050], Train Loss: 0.0114, Train Acc: 0.7398, time: 8.54
2022-08-30 12:54:23,648 Now training epoch 6. LR=0.000100
2022-08-30 12:54:23,877 Epoch[006/050], Step[0000/0079], Avg Loss: 0.0072, Avg Acc: 0.8125
2022-08-30 12:54:26,009 Epoch[006/050], Step[0020/0079], Avg Loss: 0.0109, Avg Acc: 0.7522
2022-08-30 12:54:28,140 Epoch[006/050], Step[0040/0079], Avg Loss: 0.0110, Avg Acc: 0.7553
2022-08-30 12:54:30,274 Epoch[006/050], Step[0060/0079], Avg Loss: 0.0108, Avg Acc: 0.7602
2022-08-30 12:54:32,210 ----- Epoch[006/050], Train Loss: 0.0110, Train Acc: 0.7610, time: 8.56
2022-08-30 12:54:32,210 Now training epoch 7. LR=0.000100
2022-08-30 12:54:32,441 Epoch[007/050], Step[0000/0079], Avg Loss: 0.0084, Avg Acc: 0.8125
2022-08-30 12:54:34,575 Epoch[007/050], Step[0020/0079], Avg Loss: 0.0101, Avg Acc: 0.7746
2022-08-30 12:54:36,708 Epoch[007/050], Step[0040/0079], Avg Loss: 0.0098, Avg Acc: 0.7713
2022-08-30 12:54:38,859 Epoch[007/050], Step[0060/0079], Avg Loss: 0.0095, Avg Acc: 0.7777
2022-08-30 12:54:40,809 ----- Epoch[007/050], Train Loss: 0.0099, Train Acc: 0.7748, time: 8.60
2022-08-30 12:54:40,809 Now training epoch 8. LR=0.000100
2022-08-30 12:54:41,033 Epoch[008/050], Step[0000/0079], Avg Loss: 0.0088, Avg Acc: 0.7500
2022-08-30 12:54:43,193 Epoch[008/050], Step[0020/0079], Avg Loss: 0.0094, Avg Acc: 0.7961
2022-08-30 12:54:45,350 Epoch[008/050], Step[0040/0079], Avg Loss: 0.0098, Avg Acc: 0.7870
2022-08-30 12:54:47,507 Epoch[008/050], Step[0060/0079], Avg Loss: 0.0096, Avg Acc: 0.7871
2022-08-30 12:54:49,459 ----- Epoch[008/050], Train Loss: 0.0098, Train Acc: 0.7872, time: 8.65
2022-08-30 12:54:49,459 Now training epoch 9. LR=0.000100
2022-08-30 12:54:49,703 Epoch[009/050], Step[0000/0079], Avg Loss: 0.0074, Avg Acc: 0.8125
2022-08-30 12:54:51,855 Epoch[009/050], Step[0020/0079], Avg Loss: 0.0091, Avg Acc: 0.7842
2022-08-30 12:54:54,004 Epoch[009/050], Step[0040/0079], Avg Loss: 0.0089, Avg Acc: 0.7904
2022-08-30 12:54:56,162 Epoch[009/050], Step[0060/0079], Avg Loss: 0.0087, Avg Acc: 0.7987
2022-08-30 12:54:58,103 ----- Epoch[009/050], Train Loss: 0.0087, Train Acc: 0.8030, time: 8.64
2022-08-30 12:54:58,103 Now training epoch 10. LR=0.000100
2022-08-30 12:54:58,345 Epoch[010/050], Step[0000/0079], Avg Loss: 0.0088, Avg Acc: 0.8281
2022-08-30 12:55:00,482 Epoch[010/050], Step[0020/0079], Avg Loss: 0.0083, Avg Acc: 0.8162
2022-08-30 12:55:02,618 Epoch[010/050], Step[0040/0079], Avg Loss: 0.0083, Avg Acc: 0.8121
2022-08-30 12:55:04,769 Epoch[010/050], Step[0060/0079], Avg Loss: 0.0082, Avg Acc: 0.8135
2022-08-30 12:55:06,719 ----- Epoch[010/050], Train Loss: 0.0083, Train Acc: 0.8132, time: 8.61
2022-08-30 12:55:06,720 Now training epoch 11. LR=0.000100
2022-08-30 12:55:06,940 Epoch[011/050], Step[0000/0079], Avg Loss: 0.0076, Avg Acc: 0.8438
2022-08-30 12:55:09,076 Epoch[011/050], Step[0020/0079], Avg Loss: 0.0080, Avg Acc: 0.8207
2022-08-30 12:55:11,206 Epoch[011/050], Step[0040/0079], Avg Loss: 0.0083, Avg Acc: 0.8152
2022-08-30 12:55:13,344 Epoch[011/050], Step[0060/0079], Avg Loss: 0.0079, Avg Acc: 0.8240
2022-08-30 12:55:15,280 ----- Epoch[011/050], Train Loss: 0.0081, Train Acc: 0.8208, time: 8.56
2022-08-30 12:55:15,280 Now training epoch 12. LR=0.000100
2022-08-30 12:55:15,515 Epoch[012/050], Step[0000/0079], Avg Loss: 0.0062, Avg Acc: 0.8281
2022-08-30 12:55:17,649 Epoch[012/050], Step[0020/0079], Avg Loss: 0.0085, Avg Acc: 0.7924
2022-08-30 12:55:19,779 Epoch[012/050], Step[0040/0079], Avg Loss: 0.0084, Avg Acc: 0.8018
2022-08-30 12:55:21,917 Epoch[012/050], Step[0060/0079], Avg Loss: 0.0080, Avg Acc: 0.8156
2022-08-30 12:55:23,851 ----- Epoch[012/050], Train Loss: 0.0078, Train Acc: 0.8190, time: 8.57
2022-08-30 12:55:23,852 Now training epoch 13. LR=0.000100
2022-08-30 12:55:24,091 Epoch[013/050], Step[0000/0079], Avg Loss: 0.0062, Avg Acc: 0.8906
2022-08-30 12:55:26,251 Epoch[013/050], Step[0020/0079], Avg Loss: 0.0068, Avg Acc: 0.8430
2022-08-30 12:55:28,474 Epoch[013/050], Step[0040/0079], Avg Loss: 0.0071, Avg Acc: 0.8369
2022-08-30 12:55:30,614 Epoch[013/050], Step[0060/0079], Avg Loss: 0.0072, Avg Acc: 0.8363
2022-08-30 12:55:32,557 ----- Epoch[013/050], Train Loss: 0.0075, Train Acc: 0.8334, time: 8.70
2022-08-30 12:55:32,557 Now training epoch 14. LR=0.000100
2022-08-30 12:55:32,783 Epoch[014/050], Step[0000/0079], Avg Loss: 0.0078, Avg Acc: 0.7969
2022-08-30 12:55:34,919 Epoch[014/050], Step[0020/0079], Avg Loss: 0.0066, Avg Acc: 0.8460
2022-08-30 12:55:37,063 Epoch[014/050], Step[0040/0079], Avg Loss: 0.0069, Avg Acc: 0.8365
2022-08-30 12:55:39,193 Epoch[014/050], Step[0060/0079], Avg Loss: 0.0068, Avg Acc: 0.8438
2022-08-30 12:55:41,130 ----- Epoch[014/050], Train Loss: 0.0071, Train Acc: 0.8414, time: 8.57
2022-08-30 12:55:41,130 Now training epoch 15. LR=0.000100
2022-08-30 12:55:41,357 Epoch[015/050], Step[0000/0079], Avg Loss: 0.0057, Avg Acc: 0.8438
2022-08-30 12:55:43,482 Epoch[015/050], Step[0020/0079], Avg Loss: 0.0072, Avg Acc: 0.8371
2022-08-30 12:55:45,607 Epoch[015/050], Step[0040/0079], Avg Loss: 0.0070, Avg Acc: 0.8464
2022-08-30 12:55:47,735 Epoch[015/050], Step[0060/0079], Avg Loss: 0.0070, Avg Acc: 0.8435
2022-08-30 12:55:49,680 ----- Epoch[015/050], Train Loss: 0.0072, Train Acc: 0.8454, time: 8.55
2022-08-30 12:55:49,680 Now training epoch 16. LR=0.000100
2022-08-30 12:55:49,897 Epoch[016/050], Step[0000/0079], Avg Loss: 0.0064, Avg Acc: 0.8750
2022-08-30 12:55:52,023 Epoch[016/050], Step[0020/0079], Avg Loss: 0.0081, Avg Acc: 0.8177
2022-08-30 12:55:54,157 Epoch[016/050], Step[0040/0079], Avg Loss: 0.0077, Avg Acc: 0.8293
2022-08-30 12:55:56,299 Epoch[016/050], Step[0060/0079], Avg Loss: 0.0072, Avg Acc: 0.8422
2022-08-30 12:55:58,243 ----- Epoch[016/050], Train Loss: 0.0070, Train Acc: 0.8474, time: 8.56
2022-08-30 12:55:58,243 Now training epoch 17. LR=0.000100
2022-08-30 12:55:58,471 Epoch[017/050], Step[0000/0079], Avg Loss: 0.0053, Avg Acc: 0.9219
2022-08-30 12:56:00,616 Epoch[017/050], Step[0020/0079], Avg Loss: 0.0059, Avg Acc: 0.8624
2022-08-30 12:56:02,753 Epoch[017/050], Step[0040/0079], Avg Loss: 0.0060, Avg Acc: 0.8659
2022-08-30 12:56:04,908 Epoch[017/050], Step[0060/0079], Avg Loss: 0.0061, Avg Acc: 0.8650
2022-08-30 12:56:06,854 ----- Epoch[017/050], Train Loss: 0.0062, Train Acc: 0.8622, time: 8.61
2022-08-30 12:56:06,854 Now training epoch 18. LR=0.000100
2022-08-30 12:56:07,091 Epoch[018/050], Step[0000/0079], Avg Loss: 0.0045, Avg Acc: 0.9219
2022-08-30 12:56:09,223 Epoch[018/050], Step[0020/0079], Avg Loss: 0.0067, Avg Acc: 0.8460
2022-08-30 12:56:11,369 Epoch[018/050], Step[0040/0079], Avg Loss: 0.0061, Avg Acc: 0.8579
2022-08-30 12:56:13,515 Epoch[018/050], Step[0060/0079], Avg Loss: 0.0058, Avg Acc: 0.8668
2022-08-30 12:56:15,454 ----- Epoch[018/050], Train Loss: 0.0058, Train Acc: 0.8674, time: 8.60
2022-08-30 12:56:15,454 Now training epoch 19. LR=0.000100
2022-08-30 12:56:15,673 Epoch[019/050], Step[0000/0079], Avg Loss: 0.0067, Avg Acc: 0.8750
2022-08-30 12:56:17,810 Epoch[019/050], Step[0020/0079], Avg Loss: 0.0049, Avg Acc: 0.8981
2022-08-30 12:56:19,944 Epoch[019/050], Step[0040/0079], Avg Loss: 0.0051, Avg Acc: 0.8906
2022-08-30 12:56:22,095 Epoch[019/050], Step[0060/0079], Avg Loss: 0.0054, Avg Acc: 0.8801
2022-08-30 12:56:24,047 ----- Epoch[019/050], Train Loss: 0.0055, Train Acc: 0.8770, time: 8.59
2022-08-30 12:56:24,047 Now training epoch 20. LR=0.000100
2022-08-30 12:56:24,279 Epoch[020/050], Step[0000/0079], Avg Loss: 0.0056, Avg Acc: 0.8438
2022-08-30 12:56:26,415 Epoch[020/050], Step[0020/0079], Avg Loss: 0.0051, Avg Acc: 0.8787
2022-08-30 12:56:28,554 Epoch[020/050], Step[0040/0079], Avg Loss: 0.0049, Avg Acc: 0.8883
2022-08-30 12:56:30,707 Epoch[020/050], Step[0060/0079], Avg Loss: 0.0053, Avg Acc: 0.8858
2022-08-30 12:56:32,644 ----- Epoch[020/050], Train Loss: 0.0052, Train Acc: 0.8884, time: 8.60
2022-08-30 12:56:32,644 Now training epoch 21. LR=0.000100
2022-08-30 12:56:32,886 Epoch[021/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9219
2022-08-30 12:56:35,028 Epoch[021/050], Step[0020/0079], Avg Loss: 0.0054, Avg Acc: 0.8765
2022-08-30 12:56:37,174 Epoch[021/050], Step[0040/0079], Avg Loss: 0.0050, Avg Acc: 0.8807
2022-08-30 12:56:39,309 Epoch[021/050], Step[0060/0079], Avg Loss: 0.0051, Avg Acc: 0.8814
2022-08-30 12:56:41,254 ----- Epoch[021/050], Train Loss: 0.0051, Train Acc: 0.8850, time: 8.61
2022-08-30 12:56:41,254 Now training epoch 22. LR=0.000100
2022-08-30 12:56:41,476 Epoch[022/050], Step[0000/0079], Avg Loss: 0.0049, Avg Acc: 0.9219
2022-08-30 12:56:43,606 Epoch[022/050], Step[0020/0079], Avg Loss: 0.0052, Avg Acc: 0.8847
2022-08-30 12:56:45,744 Epoch[022/050], Step[0040/0079], Avg Loss: 0.0054, Avg Acc: 0.8815
2022-08-30 12:56:47,881 Epoch[022/050], Step[0060/0079], Avg Loss: 0.0053, Avg Acc: 0.8827
2022-08-30 12:56:49,820 ----- Epoch[022/050], Train Loss: 0.0054, Train Acc: 0.8872, time: 8.56
2022-08-30 12:56:49,820 Now training epoch 23. LR=0.000100
2022-08-30 12:56:50,050 Epoch[023/050], Step[0000/0079], Avg Loss: 0.0048, Avg Acc: 0.9062
2022-08-30 12:56:52,179 Epoch[023/050], Step[0020/0079], Avg Loss: 0.0073, Avg Acc: 0.8304
2022-08-30 12:56:54,314 Epoch[023/050], Step[0040/0079], Avg Loss: 0.0062, Avg Acc: 0.8639
2022-08-30 12:56:56,442 Epoch[023/050], Step[0060/0079], Avg Loss: 0.0059, Avg Acc: 0.8717
2022-08-30 12:56:58,396 ----- Epoch[023/050], Train Loss: 0.0058, Train Acc: 0.8758, time: 8.57
2022-08-30 12:56:58,396 Now training epoch 24. LR=0.000100
2022-08-30 12:56:58,620 Epoch[024/050], Step[0000/0079], Avg Loss: 0.0028, Avg Acc: 0.9219
2022-08-30 12:57:00,749 Epoch[024/050], Step[0020/0079], Avg Loss: 0.0045, Avg Acc: 0.8981
2022-08-30 12:57:02,886 Epoch[024/050], Step[0040/0079], Avg Loss: 0.0045, Avg Acc: 0.8990
2022-08-30 12:57:05,023 Epoch[024/050], Step[0060/0079], Avg Loss: 0.0043, Avg Acc: 0.9037
2022-08-30 12:57:06,970 ----- Epoch[024/050], Train Loss: 0.0043, Train Acc: 0.9058, time: 8.57
2022-08-30 12:57:06,970 Now training epoch 25. LR=0.000100
2022-08-30 12:57:07,202 Epoch[025/050], Step[0000/0079], Avg Loss: 0.0034, Avg Acc: 0.9219
2022-08-30 12:57:09,337 Epoch[025/050], Step[0020/0079], Avg Loss: 0.0036, Avg Acc: 0.9293
2022-08-30 12:57:11,469 Epoch[025/050], Step[0040/0079], Avg Loss: 0.0039, Avg Acc: 0.9192
2022-08-30 12:57:13,603 Epoch[025/050], Step[0060/0079], Avg Loss: 0.0040, Avg Acc: 0.9144
2022-08-30 12:57:15,540 ----- Epoch[025/050], Train Loss: 0.0041, Train Acc: 0.9128, time: 8.57
2022-08-30 12:57:19,982 ----- Save model: ./outputs2/train-20220830-12-53-37/Epoch-25-Loss-0.004111099776625634.pth
2022-08-30 12:57:19,982 Now training epoch 26. LR=0.000100
2022-08-30 12:57:20,328 Epoch[026/050], Step[0000/0079], Avg Loss: 0.0039, Avg Acc: 0.9375
2022-08-30 12:57:22,485 Epoch[026/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9077
2022-08-30 12:57:24,611 Epoch[026/050], Step[0040/0079], Avg Loss: 0.0042, Avg Acc: 0.9085
2022-08-30 12:57:26,741 Epoch[026/050], Step[0060/0079], Avg Loss: 0.0040, Avg Acc: 0.9132
2022-08-30 12:57:28,688 ----- Epoch[026/050], Train Loss: 0.0043, Train Acc: 0.9088, time: 8.70
2022-08-30 12:57:28,688 Now training epoch 27. LR=0.000100
2022-08-30 12:57:28,938 Epoch[027/050], Step[0000/0079], Avg Loss: 0.0037, Avg Acc: 0.9062
2022-08-30 12:57:31,073 Epoch[027/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9092
2022-08-30 12:57:33,208 Epoch[027/050], Step[0040/0079], Avg Loss: 0.0047, Avg Acc: 0.9028
2022-08-30 12:57:35,336 Epoch[027/050], Step[0060/0079], Avg Loss: 0.0045, Avg Acc: 0.9027
2022-08-30 12:57:37,282 ----- Epoch[027/050], Train Loss: 0.0044, Train Acc: 0.9046, time: 8.59
2022-08-30 12:57:37,283 Now training epoch 28. LR=0.000100
2022-08-30 12:57:37,529 Epoch[028/050], Step[0000/0079], Avg Loss: 0.0021, Avg Acc: 0.9531
2022-08-30 12:57:39,662 Epoch[028/050], Step[0020/0079], Avg Loss: 0.0035, Avg Acc: 0.9241
2022-08-30 12:57:41,820 Epoch[028/050], Step[0040/0079], Avg Loss: 0.0034, Avg Acc: 0.9284
2022-08-30 12:57:44,034 Epoch[028/050], Step[0060/0079], Avg Loss: 0.0034, Avg Acc: 0.9262
2022-08-30 12:57:46,018 ----- Epoch[028/050], Train Loss: 0.0038, Train Acc: 0.9200, time: 8.73
2022-08-30 12:57:46,019 Now training epoch 29. LR=0.000100
2022-08-30 12:57:46,260 Epoch[029/050], Step[0000/0079], Avg Loss: 0.0037, Avg Acc: 0.9375
2022-08-30 12:57:48,400 Epoch[029/050], Step[0020/0079], Avg Loss: 0.0042, Avg Acc: 0.9196
2022-08-30 12:57:50,533 Epoch[029/050], Step[0040/0079], Avg Loss: 0.0041, Avg Acc: 0.9196
2022-08-30 12:57:52,680 Epoch[029/050], Step[0060/0079], Avg Loss: 0.0042, Avg Acc: 0.9162
2022-08-30 12:57:54,641 ----- Epoch[029/050], Train Loss: 0.0040, Train Acc: 0.9192, time: 8.62
2022-08-30 12:57:54,641 Now training epoch 30. LR=0.000100
2022-08-30 12:57:54,897 Epoch[030/050], Step[0000/0079], Avg Loss: 0.0035, Avg Acc: 0.9062
2022-08-30 12:57:57,038 Epoch[030/050], Step[0020/0079], Avg Loss: 0.0039, Avg Acc: 0.9137
2022-08-30 12:57:59,173 Epoch[030/050], Step[0040/0079], Avg Loss: 0.0036, Avg Acc: 0.9177
2022-08-30 12:58:01,311 Epoch[030/050], Step[0060/0079], Avg Loss: 0.0034, Avg Acc: 0.9247
2022-08-30 12:58:03,267 ----- Epoch[030/050], Train Loss: 0.0035, Train Acc: 0.9230, time: 8.62
2022-08-30 12:58:03,267 Now training epoch 31. LR=0.000100
2022-08-30 12:58:03,501 Epoch[031/050], Step[0000/0079], Avg Loss: 0.0055, Avg Acc: 0.8438
2022-08-30 12:58:05,633 Epoch[031/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9040
2022-08-30 12:58:07,769 Epoch[031/050], Step[0040/0079], Avg Loss: 0.0039, Avg Acc: 0.9169
2022-08-30 12:58:09,914 Epoch[031/050], Step[0060/0079], Avg Loss: 0.0039, Avg Acc: 0.9168
2022-08-30 12:58:11,879 ----- Epoch[031/050], Train Loss: 0.0040, Train Acc: 0.9122, time: 8.61
2022-08-30 12:58:11,880 Now training epoch 32. LR=0.000100
2022-08-30 12:58:12,117 Epoch[032/050], Step[0000/0079], Avg Loss: 0.0024, Avg Acc: 0.9375
2022-08-30 12:58:14,249 Epoch[032/050], Step[0020/0079], Avg Loss: 0.0032, Avg Acc: 0.9256
2022-08-30 12:58:16,382 Epoch[032/050], Step[0040/0079], Avg Loss: 0.0034, Avg Acc: 0.9249
2022-08-30 12:58:18,511 Epoch[032/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9283
2022-08-30 12:58:20,458 ----- Epoch[032/050], Train Loss: 0.0033, Train Acc: 0.9280, time: 8.58
2022-08-30 12:58:20,458 Now training epoch 33. LR=0.000100
2022-08-30 12:58:20,699 Epoch[033/050], Step[0000/0079], Avg Loss: 0.0016, Avg Acc: 0.9688
2022-08-30 12:58:22,835 Epoch[033/050], Step[0020/0079], Avg Loss: 0.0032, Avg Acc: 0.9405
2022-08-30 12:58:24,973 Epoch[033/050], Step[0040/0079], Avg Loss: 0.0031, Avg Acc: 0.9379
2022-08-30 12:58:27,110 Epoch[033/050], Step[0060/0079], Avg Loss: 0.0030, Avg Acc: 0.9372
2022-08-30 12:58:29,071 ----- Epoch[033/050], Train Loss: 0.0029, Train Acc: 0.9358, time: 8.61
2022-08-30 12:58:29,072 Now training epoch 34. LR=0.000100
2022-08-30 12:58:29,319 Epoch[034/050], Step[0000/0079], Avg Loss: 0.0017, Avg Acc: 0.9688
2022-08-30 12:58:31,450 Epoch[034/050], Step[0020/0079], Avg Loss: 0.0030, Avg Acc: 0.9338
2022-08-30 12:58:33,584 Epoch[034/050], Step[0040/0079], Avg Loss: 0.0033, Avg Acc: 0.9238
2022-08-30 12:58:35,714 Epoch[034/050], Step[0060/0079], Avg Loss: 0.0032, Avg Acc: 0.9283
2022-08-30 12:58:37,657 ----- Epoch[034/050], Train Loss: 0.0031, Train Acc: 0.9306, time: 8.58
2022-08-30 12:58:37,657 Now training epoch 35. LR=0.000100
2022-08-30 12:58:37,892 Epoch[035/050], Step[0000/0079], Avg Loss: 0.0024, Avg Acc: 0.9375
2022-08-30 12:58:40,018 Epoch[035/050], Step[0020/0079], Avg Loss: 0.0030, Avg Acc: 0.9368
2022-08-30 12:58:42,147 Epoch[035/050], Step[0040/0079], Avg Loss: 0.0029, Avg Acc: 0.9394
2022-08-30 12:58:44,287 Epoch[035/050], Step[0060/0079], Avg Loss: 0.0031, Avg Acc: 0.9367
2022-08-30 12:58:46,249 ----- Epoch[035/050], Train Loss: 0.0036, Train Acc: 0.9282, time: 8.59
2022-08-30 12:58:46,249 Now training epoch 36. LR=0.000100
2022-08-30 12:58:46,495 Epoch[036/050], Step[0000/0079], Avg Loss: 0.0014, Avg Acc: 0.9844
2022-08-30 12:58:48,627 Epoch[036/050], Step[0020/0079], Avg Loss: 0.0041, Avg Acc: 0.9204
2022-08-30 12:58:50,755 Epoch[036/050], Step[0040/0079], Avg Loss: 0.0036, Avg Acc: 0.9268
2022-08-30 12:58:52,886 Epoch[036/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9306
2022-08-30 12:58:54,837 ----- Epoch[036/050], Train Loss: 0.0034, Train Acc: 0.9294, time: 8.59
2022-08-30 12:58:54,837 Now training epoch 37. LR=0.000100
2022-08-30 12:58:55,077 Epoch[037/050], Step[0000/0079], Avg Loss: 0.0026, Avg Acc: 0.9844
2022-08-30 12:58:57,205 Epoch[037/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9412
2022-08-30 12:58:59,343 Epoch[037/050], Step[0040/0079], Avg Loss: 0.0030, Avg Acc: 0.9364
2022-08-30 12:59:01,487 Epoch[037/050], Step[0060/0079], Avg Loss: 0.0029, Avg Acc: 0.9383
2022-08-30 12:59:03,438 ----- Epoch[037/050], Train Loss: 0.0029, Train Acc: 0.9402, time: 8.60
2022-08-30 12:59:03,439 Now training epoch 38. LR=0.000100
2022-08-30 12:59:03,682 Epoch[038/050], Step[0000/0079], Avg Loss: 0.0029, Avg Acc: 0.9219
2022-08-30 12:59:05,815 Epoch[038/050], Step[0020/0079], Avg Loss: 0.0035, Avg Acc: 0.9315
2022-08-30 12:59:07,956 Epoch[038/050], Step[0040/0079], Avg Loss: 0.0035, Avg Acc: 0.9280
2022-08-30 12:59:10,099 Epoch[038/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9329
2022-08-30 12:59:12,048 ----- Epoch[038/050], Train Loss: 0.0031, Train Acc: 0.9384, time: 8.61
2022-08-30 12:59:12,048 Now training epoch 39. LR=0.000100
2022-08-30 12:59:12,283 Epoch[039/050], Step[0000/0079], Avg Loss: 0.0018, Avg Acc: 0.9688
2022-08-30 12:59:14,416 Epoch[039/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9323
2022-08-30 12:59:16,547 Epoch[039/050], Step[0040/0079], Avg Loss: 0.0029, Avg Acc: 0.9318
2022-08-30 12:59:18,678 Epoch[039/050], Step[0060/0079], Avg Loss: 0.0029, Avg Acc: 0.9342
2022-08-30 12:59:20,632 ----- Epoch[039/050], Train Loss: 0.0030, Train Acc: 0.9340, time: 8.58
2022-08-30 12:59:20,633 Now training epoch 40. LR=0.000100
2022-08-30 12:59:20,868 Epoch[040/050], Step[0000/0079], Avg Loss: 0.0033, Avg Acc: 0.9375
2022-08-30 12:59:22,999 Epoch[040/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9583
2022-08-30 12:59:25,133 Epoch[040/050], Step[0040/0079], Avg Loss: 0.0025, Avg Acc: 0.9482
2022-08-30 12:59:27,274 Epoch[040/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9413
2022-08-30 12:59:29,231 ----- Epoch[040/050], Train Loss: 0.0029, Train Acc: 0.9418, time: 8.60
2022-08-30 12:59:29,232 Now training epoch 41. LR=0.000100
2022-08-30 12:59:29,475 Epoch[041/050], Step[0000/0079], Avg Loss: 0.0040, Avg Acc: 0.9531
2022-08-30 12:59:31,607 Epoch[041/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9554
2022-08-30 12:59:33,738 Epoch[041/050], Step[0040/0079], Avg Loss: 0.0025, Avg Acc: 0.9520
2022-08-30 12:59:35,871 Epoch[041/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9493
2022-08-30 12:59:37,824 ----- Epoch[041/050], Train Loss: 0.0024, Train Acc: 0.9514, time: 8.59
2022-08-30 12:59:37,824 Now training epoch 42. LR=0.000100
2022-08-30 12:59:38,060 Epoch[042/050], Step[0000/0079], Avg Loss: 0.0031, Avg Acc: 0.9375
2022-08-30 12:59:40,194 Epoch[042/050], Step[0020/0079], Avg Loss: 0.0022, Avg Acc: 0.9576
2022-08-30 12:59:42,322 Epoch[042/050], Step[0040/0079], Avg Loss: 0.0025, Avg Acc: 0.9516
2022-08-30 12:59:44,456 Epoch[042/050], Step[0060/0079], Avg Loss: 0.0027, Avg Acc: 0.9454
2022-08-30 12:59:46,404 ----- Epoch[042/050], Train Loss: 0.0027, Train Acc: 0.9462, time: 8.58
2022-08-30 12:59:46,405 Now training epoch 43. LR=0.000100
2022-08-30 12:59:46,632 Epoch[043/050], Step[0000/0079], Avg Loss: 0.0033, Avg Acc: 0.9531
2022-08-30 12:59:48,764 Epoch[043/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9561
2022-08-30 12:59:50,898 Epoch[043/050], Step[0040/0079], Avg Loss: 0.0020, Avg Acc: 0.9596
2022-08-30 12:59:53,029 Epoch[043/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9608
2022-08-30 12:59:54,984 ----- Epoch[043/050], Train Loss: 0.0021, Train Acc: 0.9612, time: 8.58
2022-08-30 12:59:54,984 Now training epoch 44. LR=0.000100
2022-08-30 12:59:55,239 Epoch[044/050], Step[0000/0079], Avg Loss: 0.0041, Avg Acc: 0.9219
2022-08-30 12:59:57,371 Epoch[044/050], Step[0020/0079], Avg Loss: 0.0051, Avg Acc: 0.9100
2022-08-30 12:59:59,512 Epoch[044/050], Step[0040/0079], Avg Loss: 0.0042, Avg Acc: 0.9230
2022-08-30 13:00:01,684 Epoch[044/050], Step[0060/0079], Avg Loss: 0.0038, Avg Acc: 0.9265
2022-08-30 13:00:03,717 ----- Epoch[044/050], Train Loss: 0.0035, Train Acc: 0.9308, time: 8.73
2022-08-30 13:00:03,717 Now training epoch 45. LR=0.000100
2022-08-30 13:00:03,951 Epoch[045/050], Step[0000/0079], Avg Loss: 0.0023, Avg Acc: 0.9375
2022-08-30 13:00:06,082 Epoch[045/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9516
2022-08-30 13:00:08,209 Epoch[045/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9562
2022-08-30 13:00:10,342 Epoch[045/050], Step[0060/0079], Avg Loss: 0.0022, Avg Acc: 0.9557
2022-08-30 13:00:12,296 ----- Epoch[045/050], Train Loss: 0.0025, Train Acc: 0.9518, time: 8.58
2022-08-30 13:00:12,296 Now training epoch 46. LR=0.000100
2022-08-30 13:00:12,530 Epoch[046/050], Step[0000/0079], Avg Loss: 0.0034, Avg Acc: 0.9375
2022-08-30 13:00:14,653 Epoch[046/050], Step[0020/0079], Avg Loss: 0.0022, Avg Acc: 0.9539
2022-08-30 13:00:16,779 Epoch[046/050], Step[0040/0079], Avg Loss: 0.0021, Avg Acc: 0.9535
2022-08-30 13:00:18,902 Epoch[046/050], Step[0060/0079], Avg Loss: 0.0022, Avg Acc: 0.9524
2022-08-30 13:00:20,846 ----- Epoch[046/050], Train Loss: 0.0022, Train Acc: 0.9548, time: 8.55
2022-08-30 13:00:20,847 Now training epoch 47. LR=0.000100
2022-08-30 13:00:21,090 Epoch[047/050], Step[0000/0079], Avg Loss: 0.0041, Avg Acc: 0.9219
2022-08-30 13:00:23,214 Epoch[047/050], Step[0020/0079], Avg Loss: 0.0051, Avg Acc: 0.9048
2022-08-30 13:00:25,339 Epoch[047/050], Step[0040/0079], Avg Loss: 0.0042, Avg Acc: 0.9192
2022-08-30 13:00:27,474 Epoch[047/050], Step[0060/0079], Avg Loss: 0.0039, Avg Acc: 0.9229
2022-08-30 13:00:29,417 ----- Epoch[047/050], Train Loss: 0.0037, Train Acc: 0.9300, time: 8.57
2022-08-30 13:00:29,418 Now training epoch 48. LR=0.000100
2022-08-30 13:00:29,651 Epoch[048/050], Step[0000/0079], Avg Loss: 0.0023, Avg Acc: 0.9531
2022-08-30 13:00:31,786 Epoch[048/050], Step[0020/0079], Avg Loss: 0.0026, Avg Acc: 0.9427
2022-08-30 13:00:33,914 Epoch[048/050], Step[0040/0079], Avg Loss: 0.0024, Avg Acc: 0.9466
2022-08-30 13:00:36,037 Epoch[048/050], Step[0060/0079], Avg Loss: 0.0023, Avg Acc: 0.9503
2022-08-30 13:00:37,986 ----- Epoch[048/050], Train Loss: 0.0023, Train Acc: 0.9500, time: 8.57
2022-08-30 13:00:37,986 Now training epoch 49. LR=0.000100
2022-08-30 13:00:38,238 Epoch[049/050], Step[0000/0079], Avg Loss: 0.0013, Avg Acc: 0.9531
2022-08-30 13:00:40,367 Epoch[049/050], Step[0020/0079], Avg Loss: 0.0017, Avg Acc: 0.9650
2022-08-30 13:00:42,495 Epoch[049/050], Step[0040/0079], Avg Loss: 0.0018, Avg Acc: 0.9615
2022-08-30 13:00:44,628 Epoch[049/050], Step[0060/0079], Avg Loss: 0.0019, Avg Acc: 0.9595
2022-08-30 13:00:46,579 ----- Epoch[049/050], Train Loss: 0.0020, Train Acc: 0.9574, time: 8.59
2022-08-30 13:00:46,580 Now training epoch 50. LR=0.000100
2022-08-30 13:00:46,823 Epoch[050/050], Step[0000/0079], Avg Loss: 0.0030, Avg Acc: 0.8906
2022-08-30 13:00:48,957 Epoch[050/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9487
2022-08-30 13:00:51,093 Epoch[050/050], Step[0040/0079], Avg Loss: 0.0021, Avg Acc: 0.9539
2022-08-30 13:00:53,229 Epoch[050/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9582
2022-08-30 13:00:55,182 ----- Epoch[050/050], Train Loss: 0.0021, Train Acc: 0.9570, time: 8.60
2022-08-30 13:00:59,569 ----- Save model: ./outputs2/train-20220830-12-53-37/Epoch-50-Loss-0.0021423590056598185.pth
