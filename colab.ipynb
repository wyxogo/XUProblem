{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XUProblem.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVEoGXp7Mqa9LiH8S4FzKE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wyxogo/XUProblem/blob/master/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wyxogo/XUProblem.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Z51AoWp7ZX",
        "outputId": "5df938ef-d7eb-4ede-c5dd-33f379ade979"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'XUProblem'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 85 (delta 44), reused 57 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/XUProblem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpDA6ZYuajQe",
        "outputId": "3178a3cf-fd42-4fc3-e5cd-25abff28ba4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/XUProblem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --mode=train --batch_size=64 --epochs=50 --output=\"./outputs1/\" --problem=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxaHUTGl0AnY",
        "outputId": "a398737f-ab2d-4c59-a02b-d2b608e694c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:02<00:00, 79025288.46it/s]\n",
            "Extracting ./dataset/cifar-100-python.tar.gz to ./dataset/\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:02<00:00, 241MB/s]\n",
            "0830 12:44:18 PM Now training epoch 1. LR=0.000100\n",
            "0830 12:44:29 PM Epoch[001/050], Step[0000/0079], Avg Loss: 0.0681, Avg Acc: 0.0781\n",
            "0830 12:44:31 PM Epoch[001/050], Step[0020/0079], Avg Loss: 0.0299, Avg Acc: 0.3757\n",
            "0830 12:44:33 PM Epoch[001/050], Step[0040/0079], Avg Loss: 0.0231, Avg Acc: 0.5221\n",
            "0830 12:44:35 PM Epoch[001/050], Step[0060/0079], Avg Loss: 0.0203, Avg Acc: 0.5815\n",
            "0830 12:44:37 PM ----- Epoch[001/050], Train Loss: 0.0186, Train Acc: 0.6240, time: 15.29\n",
            "0830 12:44:37 PM Now training epoch 2. LR=0.000100\n",
            "0830 12:44:37 PM Epoch[002/050], Step[0000/0079], Avg Loss: 0.0089, Avg Acc: 0.8125\n",
            "0830 12:44:39 PM Epoch[002/050], Step[0020/0079], Avg Loss: 0.0114, Avg Acc: 0.7790\n",
            "0830 12:44:41 PM Epoch[002/050], Step[0040/0079], Avg Loss: 0.0104, Avg Acc: 0.7988\n",
            "0830 12:44:43 PM Epoch[002/050], Step[0060/0079], Avg Loss: 0.0098, Avg Acc: 0.8102\n",
            "0830 12:44:45 PM ----- Epoch[002/050], Train Loss: 0.0098, Train Acc: 0.8106, time: 8.37\n",
            "0830 12:44:45 PM Now training epoch 3. LR=0.000100\n",
            "0830 12:44:46 PM Epoch[003/050], Step[0000/0079], Avg Loss: 0.0084, Avg Acc: 0.8594\n",
            "0830 12:44:48 PM Epoch[003/050], Step[0020/0079], Avg Loss: 0.0068, Avg Acc: 0.8616\n",
            "0830 12:44:50 PM Epoch[003/050], Step[0040/0079], Avg Loss: 0.0075, Avg Acc: 0.8495\n",
            "0830 12:44:52 PM Epoch[003/050], Step[0060/0079], Avg Loss: 0.0076, Avg Acc: 0.8514\n",
            "0830 12:44:54 PM ----- Epoch[003/050], Train Loss: 0.0077, Train Acc: 0.8502, time: 8.39\n",
            "0830 12:44:54 PM Now training epoch 4. LR=0.000100\n",
            "0830 12:44:54 PM Epoch[004/050], Step[0000/0079], Avg Loss: 0.0083, Avg Acc: 0.8594\n",
            "0830 12:44:56 PM Epoch[004/050], Step[0020/0079], Avg Loss: 0.0075, Avg Acc: 0.8452\n",
            "0830 12:44:58 PM Epoch[004/050], Step[0040/0079], Avg Loss: 0.0072, Avg Acc: 0.8537\n",
            "0830 12:45:00 PM Epoch[004/050], Step[0060/0079], Avg Loss: 0.0069, Avg Acc: 0.8589\n",
            "0830 12:45:02 PM ----- Epoch[004/050], Train Loss: 0.0069, Train Acc: 0.8658, time: 8.44\n",
            "0830 12:45:02 PM Now training epoch 5. LR=0.000100\n",
            "0830 12:45:02 PM Epoch[005/050], Step[0000/0079], Avg Loss: 0.0042, Avg Acc: 0.9062\n",
            "0830 12:45:05 PM Epoch[005/050], Step[0020/0079], Avg Loss: 0.0059, Avg Acc: 0.8869\n",
            "0830 12:45:07 PM Epoch[005/050], Step[0040/0079], Avg Loss: 0.0059, Avg Acc: 0.8784\n",
            "0830 12:45:09 PM Epoch[005/050], Step[0060/0079], Avg Loss: 0.0056, Avg Acc: 0.8842\n",
            "0830 12:45:11 PM ----- Epoch[005/050], Train Loss: 0.0057, Train Acc: 0.8890, time: 8.48\n",
            "0830 12:45:11 PM Now training epoch 6. LR=0.000100\n",
            "0830 12:45:11 PM Epoch[006/050], Step[0000/0079], Avg Loss: 0.0086, Avg Acc: 0.8125\n",
            "0830 12:45:13 PM Epoch[006/050], Step[0020/0079], Avg Loss: 0.0053, Avg Acc: 0.8936\n",
            "0830 12:45:15 PM Epoch[006/050], Step[0040/0079], Avg Loss: 0.0053, Avg Acc: 0.9017\n",
            "0830 12:45:17 PM Epoch[006/050], Step[0060/0079], Avg Loss: 0.0053, Avg Acc: 0.9024\n",
            "0830 12:45:19 PM ----- Epoch[006/050], Train Loss: 0.0052, Train Acc: 0.9026, time: 8.51\n",
            "0830 12:45:19 PM Now training epoch 7. LR=0.000100\n",
            "0830 12:45:19 PM Epoch[007/050], Step[0000/0079], Avg Loss: 0.0028, Avg Acc: 0.9375\n",
            "0830 12:45:22 PM Epoch[007/050], Step[0020/0079], Avg Loss: 0.0053, Avg Acc: 0.8951\n",
            "0830 12:45:24 PM Epoch[007/050], Step[0040/0079], Avg Loss: 0.0051, Avg Acc: 0.9021\n",
            "0830 12:45:26 PM Epoch[007/050], Step[0060/0079], Avg Loss: 0.0047, Avg Acc: 0.9088\n",
            "0830 12:45:28 PM ----- Epoch[007/050], Train Loss: 0.0046, Train Acc: 0.9102, time: 8.54\n",
            "0830 12:45:28 PM Now training epoch 8. LR=0.000100\n",
            "0830 12:45:28 PM Epoch[008/050], Step[0000/0079], Avg Loss: 0.0070, Avg Acc: 0.8594\n",
            "0830 12:45:30 PM Epoch[008/050], Step[0020/0079], Avg Loss: 0.0038, Avg Acc: 0.9249\n",
            "0830 12:45:32 PM Epoch[008/050], Step[0040/0079], Avg Loss: 0.0037, Avg Acc: 0.9268\n",
            "0830 12:45:34 PM Epoch[008/050], Step[0060/0079], Avg Loss: 0.0038, Avg Acc: 0.9270\n",
            "0830 12:45:36 PM ----- Epoch[008/050], Train Loss: 0.0039, Train Acc: 0.9242, time: 8.58\n",
            "0830 12:45:36 PM Now training epoch 9. LR=0.000100\n",
            "0830 12:45:37 PM Epoch[009/050], Step[0000/0079], Avg Loss: 0.0026, Avg Acc: 0.9375\n",
            "0830 12:45:39 PM Epoch[009/050], Step[0020/0079], Avg Loss: 0.0036, Avg Acc: 0.9241\n",
            "0830 12:45:41 PM Epoch[009/050], Step[0040/0079], Avg Loss: 0.0036, Avg Acc: 0.9299\n",
            "0830 12:45:43 PM Epoch[009/050], Step[0060/0079], Avg Loss: 0.0037, Avg Acc: 0.9290\n",
            "0830 12:45:45 PM ----- Epoch[009/050], Train Loss: 0.0037, Train Acc: 0.9284, time: 8.65\n",
            "0830 12:45:45 PM Now training epoch 10. LR=0.000100\n",
            "0830 12:45:45 PM Epoch[010/050], Step[0000/0079], Avg Loss: 0.0046, Avg Acc: 0.8750\n",
            "0830 12:45:47 PM Epoch[010/050], Step[0020/0079], Avg Loss: 0.0036, Avg Acc: 0.9368\n",
            "0830 12:45:49 PM Epoch[010/050], Step[0040/0079], Avg Loss: 0.0035, Avg Acc: 0.9364\n",
            "0830 12:45:52 PM Epoch[010/050], Step[0060/0079], Avg Loss: 0.0035, Avg Acc: 0.9349\n",
            "0830 12:45:54 PM ----- Epoch[010/050], Train Loss: 0.0034, Train Acc: 0.9348, time: 8.65\n",
            "0830 12:45:54 PM Now training epoch 11. LR=0.000100\n",
            "0830 12:45:54 PM Epoch[011/050], Step[0000/0079], Avg Loss: 0.0039, Avg Acc: 0.9219\n",
            "0830 12:45:56 PM Epoch[011/050], Step[0020/0079], Avg Loss: 0.0030, Avg Acc: 0.9405\n",
            "0830 12:45:58 PM Epoch[011/050], Step[0040/0079], Avg Loss: 0.0037, Avg Acc: 0.9306\n",
            "0830 12:46:00 PM Epoch[011/050], Step[0060/0079], Avg Loss: 0.0037, Avg Acc: 0.9314\n",
            "0830 12:46:02 PM ----- Epoch[011/050], Train Loss: 0.0037, Train Acc: 0.9314, time: 8.63\n",
            "0830 12:46:02 PM Now training epoch 12. LR=0.000100\n",
            "0830 12:46:02 PM Epoch[012/050], Step[0000/0079], Avg Loss: 0.0014, Avg Acc: 0.9688\n",
            "0830 12:46:05 PM Epoch[012/050], Step[0020/0079], Avg Loss: 0.0038, Avg Acc: 0.9263\n",
            "0830 12:46:07 PM Epoch[012/050], Step[0040/0079], Avg Loss: 0.0031, Avg Acc: 0.9394\n",
            "0830 12:46:09 PM Epoch[012/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9362\n",
            "0830 12:46:11 PM ----- Epoch[012/050], Train Loss: 0.0032, Train Acc: 0.9398, time: 8.62\n",
            "0830 12:46:11 PM Now training epoch 13. LR=0.000100\n",
            "0830 12:46:11 PM Epoch[013/050], Step[0000/0079], Avg Loss: 0.0019, Avg Acc: 0.9688\n",
            "0830 12:46:13 PM Epoch[013/050], Step[0020/0079], Avg Loss: 0.0026, Avg Acc: 0.9501\n",
            "0830 12:46:15 PM Epoch[013/050], Step[0040/0079], Avg Loss: 0.0028, Avg Acc: 0.9455\n",
            "0830 12:46:18 PM Epoch[013/050], Step[0060/0079], Avg Loss: 0.0027, Avg Acc: 0.9480\n",
            "0830 12:46:19 PM ----- Epoch[013/050], Train Loss: 0.0027, Train Acc: 0.9480, time: 8.60\n",
            "0830 12:46:19 PM Now training epoch 14. LR=0.000100\n",
            "0830 12:46:20 PM Epoch[014/050], Step[0000/0079], Avg Loss: 0.0029, Avg Acc: 0.9688\n",
            "0830 12:46:22 PM Epoch[014/050], Step[0020/0079], Avg Loss: 0.0017, Avg Acc: 0.9650\n",
            "0830 12:46:24 PM Epoch[014/050], Step[0040/0079], Avg Loss: 0.0019, Avg Acc: 0.9600\n",
            "0830 12:46:26 PM Epoch[014/050], Step[0060/0079], Avg Loss: 0.0023, Avg Acc: 0.9536\n",
            "0830 12:46:28 PM ----- Epoch[014/050], Train Loss: 0.0025, Train Acc: 0.9514, time: 8.63\n",
            "0830 12:46:28 PM Now training epoch 15. LR=0.000100\n",
            "0830 12:46:28 PM Epoch[015/050], Step[0000/0079], Avg Loss: 0.0023, Avg Acc: 0.9375\n",
            "0830 12:46:30 PM Epoch[015/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9256\n",
            "0830 12:46:33 PM Epoch[015/050], Step[0040/0079], Avg Loss: 0.0038, Avg Acc: 0.9348\n",
            "0830 12:46:35 PM Epoch[015/050], Step[0060/0079], Avg Loss: 0.0035, Avg Acc: 0.9413\n",
            "0830 12:46:37 PM ----- Epoch[015/050], Train Loss: 0.0035, Train Acc: 0.9436, time: 8.56\n",
            "0830 12:46:37 PM Now training epoch 16. LR=0.000100\n",
            "0830 12:46:37 PM Epoch[016/050], Step[0000/0079], Avg Loss: 0.0012, Avg Acc: 0.9844\n",
            "0830 12:46:39 PM Epoch[016/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9501\n",
            "0830 12:46:41 PM Epoch[016/050], Step[0040/0079], Avg Loss: 0.0026, Avg Acc: 0.9554\n",
            "0830 12:46:43 PM Epoch[016/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9547\n",
            "0830 12:46:45 PM ----- Epoch[016/050], Train Loss: 0.0026, Train Acc: 0.9542, time: 8.57\n",
            "0830 12:46:45 PM Now training epoch 17. LR=0.000100\n",
            "0830 12:46:45 PM Epoch[017/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9375\n",
            "0830 12:46:48 PM Epoch[017/050], Step[0020/0079], Avg Loss: 0.0025, Avg Acc: 0.9598\n",
            "0830 12:46:50 PM Epoch[017/050], Step[0040/0079], Avg Loss: 0.0024, Avg Acc: 0.9566\n",
            "0830 12:46:52 PM Epoch[017/050], Step[0060/0079], Avg Loss: 0.0025, Avg Acc: 0.9544\n",
            "0830 12:46:54 PM ----- Epoch[017/050], Train Loss: 0.0026, Train Acc: 0.9564, time: 8.53\n",
            "0830 12:46:54 PM Now training epoch 18. LR=0.000100\n",
            "0830 12:46:54 PM Epoch[018/050], Step[0000/0079], Avg Loss: 0.0009, Avg Acc: 0.9844\n",
            "0830 12:46:56 PM Epoch[018/050], Step[0020/0079], Avg Loss: 0.0019, Avg Acc: 0.9680\n",
            "0830 12:46:58 PM Epoch[018/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9592\n",
            "0830 12:47:00 PM Epoch[018/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9639\n",
            "0830 12:47:02 PM ----- Epoch[018/050], Train Loss: 0.0023, Train Acc: 0.9620, time: 8.56\n",
            "0830 12:47:02 PM Now training epoch 19. LR=0.000100\n",
            "0830 12:47:03 PM Epoch[019/050], Step[0000/0079], Avg Loss: 0.0019, Avg Acc: 0.9688\n",
            "0830 12:47:05 PM Epoch[019/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9613\n",
            "0830 12:47:07 PM Epoch[019/050], Step[0040/0079], Avg Loss: 0.0021, Avg Acc: 0.9646\n",
            "0830 12:47:09 PM Epoch[019/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9634\n",
            "0830 12:47:11 PM ----- Epoch[019/050], Train Loss: 0.0020, Train Acc: 0.9654, time: 8.56\n",
            "0830 12:47:11 PM Now training epoch 20. LR=0.000100\n",
            "0830 12:47:11 PM Epoch[020/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9375\n",
            "0830 12:47:13 PM Epoch[020/050], Step[0020/0079], Avg Loss: 0.0020, Avg Acc: 0.9583\n",
            "0830 12:47:15 PM Epoch[020/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9573\n",
            "0830 12:47:18 PM Epoch[020/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9606\n",
            "0830 12:47:19 PM ----- Epoch[020/050], Train Loss: 0.0019, Train Acc: 0.9640, time: 8.61\n",
            "0830 12:47:19 PM Now training epoch 21. LR=0.000100\n",
            "0830 12:47:20 PM Epoch[021/050], Step[0000/0079], Avg Loss: 0.0020, Avg Acc: 0.9531\n",
            "0830 12:47:22 PM Epoch[021/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9546\n",
            "0830 12:47:24 PM Epoch[021/050], Step[0040/0079], Avg Loss: 0.0023, Avg Acc: 0.9585\n",
            "0830 12:47:26 PM Epoch[021/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9613\n",
            "0830 12:47:28 PM ----- Epoch[021/050], Train Loss: 0.0020, Train Acc: 0.9644, time: 8.58\n",
            "0830 12:47:28 PM Now training epoch 22. LR=0.000100\n",
            "0830 12:47:28 PM Epoch[022/050], Step[0000/0079], Avg Loss: 0.0002, Avg Acc: 1.0000\n",
            "0830 12:47:30 PM Epoch[022/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9807\n",
            "0830 12:47:33 PM Epoch[022/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9775\n",
            "0830 12:47:35 PM Epoch[022/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9769\n",
            "0830 12:47:37 PM ----- Epoch[022/050], Train Loss: 0.0015, Train Acc: 0.9718, time: 8.57\n",
            "0830 12:47:37 PM Now training epoch 23. LR=0.000100\n",
            "0830 12:47:37 PM Epoch[023/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9531\n",
            "0830 12:47:39 PM Epoch[023/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9591\n",
            "0830 12:47:41 PM Epoch[023/050], Step[0040/0079], Avg Loss: 0.0019, Avg Acc: 0.9668\n",
            "0830 12:47:43 PM Epoch[023/050], Step[0060/0079], Avg Loss: 0.0019, Avg Acc: 0.9662\n",
            "0830 12:47:45 PM ----- Epoch[023/050], Train Loss: 0.0020, Train Acc: 0.9676, time: 8.55\n",
            "0830 12:47:45 PM Now training epoch 24. LR=0.000100\n",
            "0830 12:47:45 PM Epoch[024/050], Step[0000/0079], Avg Loss: 0.0034, Avg Acc: 0.9375\n",
            "0830 12:47:48 PM Epoch[024/050], Step[0020/0079], Avg Loss: 0.0019, Avg Acc: 0.9665\n",
            "0830 12:47:50 PM Epoch[024/050], Step[0040/0079], Avg Loss: 0.0017, Avg Acc: 0.9699\n",
            "0830 12:47:52 PM Epoch[024/050], Step[0060/0079], Avg Loss: 0.0017, Avg Acc: 0.9698\n",
            "0830 12:47:54 PM ----- Epoch[024/050], Train Loss: 0.0017, Train Acc: 0.9694, time: 8.55\n",
            "0830 12:47:54 PM Now training epoch 25. LR=0.000100\n",
            "0830 12:47:54 PM Epoch[025/050], Step[0000/0079], Avg Loss: 0.0005, Avg Acc: 1.0000\n",
            "0830 12:47:56 PM Epoch[025/050], Step[0020/0079], Avg Loss: 0.0013, Avg Acc: 0.9732\n",
            "0830 12:47:58 PM Epoch[025/050], Step[0040/0079], Avg Loss: 0.0017, Avg Acc: 0.9684\n",
            "0830 12:48:00 PM Epoch[025/050], Step[0060/0079], Avg Loss: 0.0017, Avg Acc: 0.9682\n",
            "0830 12:48:02 PM ----- Epoch[025/050], Train Loss: 0.0019, Train Acc: 0.9686, time: 8.57\n",
            "0830 12:48:07 PM ----- Save model: ./outputs1/train-20220830-12-44-18/Epoch-25-Loss-0.0018840523378923536.pth\n",
            "0830 12:48:07 PM Now training epoch 26. LR=0.000100\n",
            "0830 12:48:07 PM Epoch[026/050], Step[0000/0079], Avg Loss: 0.0013, Avg Acc: 0.9531\n",
            "0830 12:48:09 PM Epoch[026/050], Step[0020/0079], Avg Loss: 0.0031, Avg Acc: 0.9360\n",
            "0830 12:48:11 PM Epoch[026/050], Step[0040/0079], Avg Loss: 0.0031, Avg Acc: 0.9386\n",
            "0830 12:48:14 PM Epoch[026/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9475\n",
            "0830 12:48:16 PM ----- Epoch[026/050], Train Loss: 0.0023, Train Acc: 0.9530, time: 8.64\n",
            "0830 12:48:16 PM Now training epoch 27. LR=0.000100\n",
            "0830 12:48:16 PM Epoch[027/050], Step[0000/0079], Avg Loss: 0.0002, Avg Acc: 1.0000\n",
            "0830 12:48:18 PM Epoch[027/050], Step[0020/0079], Avg Loss: 0.0012, Avg Acc: 0.9792\n",
            "0830 12:48:20 PM Epoch[027/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9806\n",
            "0830 12:48:22 PM Epoch[027/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9795\n",
            "0830 12:48:24 PM ----- Epoch[027/050], Train Loss: 0.0012, Train Acc: 0.9762, time: 8.60\n",
            "0830 12:48:24 PM Now training epoch 28. LR=0.000100\n",
            "0830 12:48:24 PM Epoch[028/050], Step[0000/0079], Avg Loss: 0.0013, Avg Acc: 0.9688\n",
            "0830 12:48:26 PM Epoch[028/050], Step[0020/0079], Avg Loss: 0.0014, Avg Acc: 0.9702\n",
            "0830 12:48:29 PM Epoch[028/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9748\n",
            "0830 12:48:31 PM Epoch[028/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9775\n",
            "0830 12:48:33 PM ----- Epoch[028/050], Train Loss: 0.0012, Train Acc: 0.9754, time: 8.58\n",
            "0830 12:48:33 PM Now training epoch 29. LR=0.000100\n",
            "0830 12:48:33 PM Epoch[029/050], Step[0000/0079], Avg Loss: 0.0025, Avg Acc: 0.9375\n",
            "0830 12:48:35 PM Epoch[029/050], Step[0020/0079], Avg Loss: 0.0016, Avg Acc: 0.9695\n",
            "0830 12:48:37 PM Epoch[029/050], Step[0040/0079], Avg Loss: 0.0014, Avg Acc: 0.9745\n",
            "0830 12:48:39 PM Epoch[029/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9728\n",
            "0830 12:48:41 PM ----- Epoch[029/050], Train Loss: 0.0016, Train Acc: 0.9722, time: 8.72\n",
            "0830 12:48:41 PM Now training epoch 30. LR=0.000100\n",
            "0830 12:48:42 PM Epoch[030/050], Step[0000/0079], Avg Loss: 0.0009, Avg Acc: 0.9531\n",
            "0830 12:48:44 PM Epoch[030/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9799\n",
            "0830 12:48:46 PM Epoch[030/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9779\n",
            "0830 12:48:48 PM Epoch[030/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9739\n",
            "0830 12:48:50 PM ----- Epoch[030/050], Train Loss: 0.0015, Train Acc: 0.9726, time: 8.59\n",
            "0830 12:48:50 PM Now training epoch 31. LR=0.000100\n",
            "0830 12:48:50 PM Epoch[031/050], Step[0000/0079], Avg Loss: 0.0018, Avg Acc: 0.9844\n",
            "0830 12:48:52 PM Epoch[031/050], Step[0020/0079], Avg Loss: 0.0013, Avg Acc: 0.9784\n",
            "0830 12:48:55 PM Epoch[031/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9832\n",
            "0830 12:48:57 PM Epoch[031/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9816\n",
            "0830 12:48:59 PM ----- Epoch[031/050], Train Loss: 0.0012, Train Acc: 0.9800, time: 8.60\n",
            "0830 12:48:59 PM Now training epoch 32. LR=0.000100\n",
            "0830 12:48:59 PM Epoch[032/050], Step[0000/0079], Avg Loss: 0.0035, Avg Acc: 0.9375\n",
            "0830 12:49:01 PM Epoch[032/050], Step[0020/0079], Avg Loss: 0.0015, Avg Acc: 0.9769\n",
            "0830 12:49:03 PM Epoch[032/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9809\n",
            "0830 12:49:05 PM Epoch[032/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9808\n",
            "0830 12:49:07 PM ----- Epoch[032/050], Train Loss: 0.0013, Train Acc: 0.9796, time: 8.60\n",
            "0830 12:49:07 PM Now training epoch 33. LR=0.000100\n",
            "0830 12:49:07 PM Epoch[033/050], Step[0000/0079], Avg Loss: 0.0018, Avg Acc: 0.9844\n",
            "0830 12:49:10 PM Epoch[033/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9821\n",
            "0830 12:49:12 PM Epoch[033/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9783\n",
            "0830 12:49:14 PM Epoch[033/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9746\n",
            "0830 12:49:16 PM ----- Epoch[033/050], Train Loss: 0.0016, Train Acc: 0.9724, time: 8.59\n",
            "0830 12:49:16 PM Now training epoch 34. LR=0.000100\n",
            "0830 12:49:16 PM Epoch[034/050], Step[0000/0079], Avg Loss: 0.0022, Avg Acc: 0.9688\n",
            "0830 12:49:18 PM Epoch[034/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9792\n",
            "0830 12:49:20 PM Epoch[034/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9809\n",
            "0830 12:49:22 PM Epoch[034/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9805\n",
            "0830 12:49:24 PM ----- Epoch[034/050], Train Loss: 0.0012, Train Acc: 0.9820, time: 8.55\n",
            "0830 12:49:24 PM Now training epoch 35. LR=0.000100\n",
            "0830 12:49:25 PM Epoch[035/050], Step[0000/0079], Avg Loss: 0.0007, Avg Acc: 0.9844\n",
            "0830 12:49:27 PM Epoch[035/050], Step[0020/0079], Avg Loss: 0.0014, Avg Acc: 0.9777\n",
            "0830 12:49:29 PM Epoch[035/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9775\n",
            "0830 12:49:31 PM Epoch[035/050], Step[0060/0079], Avg Loss: 0.0015, Avg Acc: 0.9739\n",
            "0830 12:49:33 PM ----- Epoch[035/050], Train Loss: 0.0016, Train Acc: 0.9714, time: 8.57\n",
            "0830 12:49:33 PM Now training epoch 36. LR=0.000100\n",
            "0830 12:49:33 PM Epoch[036/050], Step[0000/0079], Avg Loss: 0.0016, Avg Acc: 0.9688\n",
            "0830 12:49:35 PM Epoch[036/050], Step[0020/0079], Avg Loss: 0.0008, Avg Acc: 0.9844\n",
            "0830 12:49:37 PM Epoch[036/050], Step[0040/0079], Avg Loss: 0.0008, Avg Acc: 0.9829\n",
            "0830 12:49:40 PM Epoch[036/050], Step[0060/0079], Avg Loss: 0.0009, Avg Acc: 0.9813\n",
            "0830 12:49:42 PM ----- Epoch[036/050], Train Loss: 0.0009, Train Acc: 0.9818, time: 8.59\n",
            "0830 12:49:42 PM Now training epoch 37. LR=0.000100\n",
            "0830 12:49:42 PM Epoch[037/050], Step[0000/0079], Avg Loss: 0.0007, Avg Acc: 0.9688\n",
            "0830 12:49:44 PM Epoch[037/050], Step[0020/0079], Avg Loss: 0.0015, Avg Acc: 0.9784\n",
            "0830 12:49:46 PM Epoch[037/050], Step[0040/0079], Avg Loss: 0.0016, Avg Acc: 0.9733\n",
            "0830 12:49:48 PM Epoch[037/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9759\n",
            "0830 12:49:50 PM ----- Epoch[037/050], Train Loss: 0.0012, Train Acc: 0.9782, time: 8.59\n",
            "0830 12:49:50 PM Now training epoch 38. LR=0.000100\n",
            "0830 12:49:50 PM Epoch[038/050], Step[0000/0079], Avg Loss: 0.0000, Avg Acc: 1.0000\n",
            "0830 12:49:53 PM Epoch[038/050], Step[0020/0079], Avg Loss: 0.0007, Avg Acc: 0.9881\n",
            "0830 12:49:55 PM Epoch[038/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9809\n",
            "0830 12:49:57 PM Epoch[038/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9821\n",
            "0830 12:49:59 PM ----- Epoch[038/050], Train Loss: 0.0011, Train Acc: 0.9818, time: 8.58\n",
            "0830 12:49:59 PM Now training epoch 39. LR=0.000100\n",
            "0830 12:49:59 PM Epoch[039/050], Step[0000/0079], Avg Loss: 0.0002, Avg Acc: 1.0000\n",
            "0830 12:50:01 PM Epoch[039/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9777\n",
            "0830 12:50:03 PM Epoch[039/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9798\n",
            "0830 12:50:05 PM Epoch[039/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9798\n",
            "0830 12:50:07 PM ----- Epoch[039/050], Train Loss: 0.0012, Train Acc: 0.9790, time: 8.59\n",
            "0830 12:50:07 PM Now training epoch 40. LR=0.000100\n",
            "0830 12:50:08 PM Epoch[040/050], Step[0000/0079], Avg Loss: 0.0030, Avg Acc: 0.9531\n",
            "0830 12:50:10 PM Epoch[040/050], Step[0020/0079], Avg Loss: 0.0009, Avg Acc: 0.9866\n",
            "0830 12:50:12 PM Epoch[040/050], Step[0040/0079], Avg Loss: 0.0009, Avg Acc: 0.9886\n",
            "0830 12:50:14 PM Epoch[040/050], Step[0060/0079], Avg Loss: 0.0009, Avg Acc: 0.9874\n",
            "0830 12:50:16 PM ----- Epoch[040/050], Train Loss: 0.0008, Train Acc: 0.9882, time: 8.60\n",
            "0830 12:50:16 PM Now training epoch 41. LR=0.000100\n",
            "0830 12:50:16 PM Epoch[041/050], Step[0000/0079], Avg Loss: 0.0006, Avg Acc: 0.9844\n",
            "0830 12:50:18 PM Epoch[041/050], Step[0020/0079], Avg Loss: 0.0007, Avg Acc: 0.9881\n",
            "0830 12:50:20 PM Epoch[041/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9817\n",
            "0830 12:50:23 PM Epoch[041/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9831\n",
            "0830 12:50:24 PM ----- Epoch[041/050], Train Loss: 0.0011, Train Acc: 0.9834, time: 8.58\n",
            "0830 12:50:24 PM Now training epoch 42. LR=0.000100\n",
            "0830 12:50:25 PM Epoch[042/050], Step[0000/0079], Avg Loss: 0.0001, Avg Acc: 1.0000\n",
            "0830 12:50:27 PM Epoch[042/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9621\n",
            "0830 12:50:29 PM Epoch[042/050], Step[0040/0079], Avg Loss: 0.0023, Avg Acc: 0.9653\n",
            "0830 12:50:31 PM Epoch[042/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9688\n",
            "0830 12:50:33 PM ----- Epoch[042/050], Train Loss: 0.0020, Train Acc: 0.9698, time: 8.57\n",
            "0830 12:50:33 PM Now training epoch 43. LR=0.000100\n",
            "0830 12:50:33 PM Epoch[043/050], Step[0000/0079], Avg Loss: 0.0008, Avg Acc: 0.9844\n",
            "0830 12:50:35 PM Epoch[043/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9836\n",
            "0830 12:50:38 PM Epoch[043/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9813\n",
            "0830 12:50:40 PM Epoch[043/050], Step[0060/0079], Avg Loss: 0.0010, Avg Acc: 0.9821\n",
            "0830 12:50:42 PM ----- Epoch[043/050], Train Loss: 0.0012, Train Acc: 0.9806, time: 8.57\n",
            "0830 12:50:42 PM Now training epoch 44. LR=0.000100\n",
            "0830 12:50:42 PM Epoch[044/050], Step[0000/0079], Avg Loss: 0.0020, Avg Acc: 0.9688\n",
            "0830 12:50:44 PM Epoch[044/050], Step[0020/0079], Avg Loss: 0.0015, Avg Acc: 0.9717\n",
            "0830 12:50:46 PM Epoch[044/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9752\n",
            "0830 12:50:48 PM Epoch[044/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9759\n",
            "0830 12:50:50 PM ----- Epoch[044/050], Train Loss: 0.0013, Train Acc: 0.9770, time: 8.58\n",
            "0830 12:50:50 PM Now training epoch 45. LR=0.000100\n",
            "0830 12:50:50 PM Epoch[045/050], Step[0000/0079], Avg Loss: 0.0005, Avg Acc: 1.0000\n",
            "0830 12:50:53 PM Epoch[045/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9792\n",
            "0830 12:50:55 PM Epoch[045/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9783\n",
            "0830 12:50:57 PM Epoch[045/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9816\n",
            "0830 12:50:59 PM ----- Epoch[045/050], Train Loss: 0.0011, Train Acc: 0.9816, time: 8.69\n",
            "0830 12:50:59 PM Now training epoch 46. LR=0.000100\n",
            "0830 12:50:59 PM Epoch[046/050], Step[0000/0079], Avg Loss: 0.0001, Avg Acc: 1.0000\n",
            "0830 12:51:01 PM Epoch[046/050], Step[0020/0079], Avg Loss: 0.0016, Avg Acc: 0.9799\n",
            "0830 12:51:03 PM Epoch[046/050], Step[0040/0079], Avg Loss: 0.0014, Avg Acc: 0.9790\n",
            "0830 12:51:06 PM Epoch[046/050], Step[0060/0079], Avg Loss: 0.0015, Avg Acc: 0.9798\n",
            "0830 12:51:07 PM ----- Epoch[046/050], Train Loss: 0.0013, Train Acc: 0.9804, time: 8.58\n",
            "0830 12:51:07 PM Now training epoch 47. LR=0.000100\n",
            "0830 12:51:08 PM Epoch[047/050], Step[0000/0079], Avg Loss: 0.0026, Avg Acc: 0.9844\n",
            "0830 12:51:10 PM Epoch[047/050], Step[0020/0079], Avg Loss: 0.0012, Avg Acc: 0.9814\n",
            "0830 12:51:12 PM Epoch[047/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9806\n",
            "0830 12:51:14 PM Epoch[047/050], Step[0060/0079], Avg Loss: 0.0010, Avg Acc: 0.9826\n",
            "0830 12:51:16 PM ----- Epoch[047/050], Train Loss: 0.0010, Train Acc: 0.9832, time: 8.58\n",
            "0830 12:51:16 PM Now training epoch 48. LR=0.000100\n",
            "0830 12:51:16 PM Epoch[048/050], Step[0000/0079], Avg Loss: 0.0033, Avg Acc: 0.9375\n",
            "0830 12:51:18 PM Epoch[048/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9688\n",
            "0830 12:51:21 PM Epoch[048/050], Step[0040/0079], Avg Loss: 0.0016, Avg Acc: 0.9771\n",
            "0830 12:51:23 PM Epoch[048/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9787\n",
            "0830 12:51:25 PM ----- Epoch[048/050], Train Loss: 0.0015, Train Acc: 0.9782, time: 8.58\n",
            "0830 12:51:25 PM Now training epoch 49. LR=0.000100\n",
            "0830 12:51:25 PM Epoch[049/050], Step[0000/0079], Avg Loss: 0.0003, Avg Acc: 1.0000\n",
            "0830 12:51:27 PM Epoch[049/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9836\n",
            "0830 12:51:29 PM Epoch[049/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9806\n",
            "0830 12:51:31 PM Epoch[049/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9795\n",
            "0830 12:51:33 PM ----- Epoch[049/050], Train Loss: 0.0011, Train Acc: 0.9796, time: 8.56\n",
            "0830 12:51:33 PM Now training epoch 50. LR=0.000100\n",
            "0830 12:51:33 PM Epoch[050/050], Step[0000/0079], Avg Loss: 0.0041, Avg Acc: 0.9375\n",
            "0830 12:51:36 PM Epoch[050/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9814\n",
            "0830 12:51:38 PM Epoch[050/050], Step[0040/0079], Avg Loss: 0.0009, Avg Acc: 0.9863\n",
            "0830 12:51:40 PM Epoch[050/050], Step[0060/0079], Avg Loss: 0.0007, Avg Acc: 0.9892\n",
            "0830 12:51:42 PM ----- Epoch[050/050], Train Loss: 0.0007, Train Acc: 0.9896, time: 8.58\n",
            "0830 12:51:46 PM ----- Save model: ./outputs1/train-20220830-12-44-18/Epoch-50-Loss-0.000734963038371643.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --mode=test --batch_size=32 --output=\"./outputs1/\" --problem=1"
      ],
      "metadata": {
        "id": "MXWg0B8rroIa",
        "outputId": "85dd2c0e-0d1f-442e-e97a-e800ae9a7833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "0830 12:53:22 PM ----- Start Test\n",
            "0830 12:53:26 PM Test Step[0000/0032], Avg Loss: 0.0089, Avg Test Acc: 0.9375\n",
            "0830 12:53:27 PM Test Step[0010/0032], Avg Loss: 0.0118, Avg Test Acc: 0.9375\n",
            "0830 12:53:27 PM Test Step[0020/0032], Avg Loss: 0.0193, Avg Test Acc: 0.9286\n",
            "0830 12:53:27 PM Test Step[0030/0032], Avg Loss: 0.0170, Avg Test Acc: 0.9244\n",
            "0830 12:53:27 PM Test Loss: 0.0177, Test Acc: 0.9240, time: 2.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --mode=train --batch_size=64 --epochs=50 --output=\"./outputs2/\" --problem=2"
      ],
      "metadata": {
        "id": "96cjRMapyGno",
        "outputId": "df50b1f4-4baa-474b-b9a4-3b8f780d1898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "0830 12:53:37 PM Now training epoch 1. LR=0.000100\n",
            "0830 12:53:41 PM Epoch[001/050], Step[0000/0079], Avg Loss: 0.0656, Avg Acc: 0.0469\n",
            "0830 12:53:43 PM Epoch[001/050], Step[0020/0079], Avg Loss: 0.0326, Avg Acc: 0.2746\n",
            "0830 12:53:45 PM Epoch[001/050], Step[0040/0079], Avg Loss: 0.0274, Avg Acc: 0.3685\n",
            "0830 12:53:47 PM Epoch[001/050], Step[0060/0079], Avg Loss: 0.0245, Avg Acc: 0.4370\n",
            "0830 12:53:49 PM ----- Epoch[001/050], Train Loss: 0.0231, Train Acc: 0.4680, time: 10.42\n",
            "0830 12:53:49 PM Now training epoch 2. LR=0.000100\n",
            "0830 12:53:49 PM Epoch[002/050], Step[0000/0079], Avg Loss: 0.0168, Avg Acc: 0.6719\n",
            "0830 12:53:52 PM Epoch[002/050], Step[0020/0079], Avg Loss: 0.0159, Avg Acc: 0.6257\n",
            "0830 12:53:54 PM Epoch[002/050], Step[0040/0079], Avg Loss: 0.0156, Avg Acc: 0.6345\n",
            "0830 12:53:56 PM Epoch[002/050], Step[0060/0079], Avg Loss: 0.0159, Avg Acc: 0.6296\n",
            "0830 12:53:58 PM ----- Epoch[002/050], Train Loss: 0.0157, Train Acc: 0.6362, time: 8.40\n",
            "0830 12:53:58 PM Now training epoch 3. LR=0.000100\n",
            "0830 12:53:58 PM Epoch[003/050], Step[0000/0079], Avg Loss: 0.0150, Avg Acc: 0.6406\n",
            "0830 12:54:00 PM Epoch[003/050], Step[0020/0079], Avg Loss: 0.0137, Avg Acc: 0.6749\n",
            "0830 12:54:02 PM Epoch[003/050], Step[0040/0079], Avg Loss: 0.0140, Avg Acc: 0.6715\n",
            "0830 12:54:04 PM Epoch[003/050], Step[0060/0079], Avg Loss: 0.0139, Avg Acc: 0.6770\n",
            "0830 12:54:06 PM ----- Epoch[003/050], Train Loss: 0.0138, Train Acc: 0.6838, time: 8.47\n",
            "0830 12:54:06 PM Now training epoch 4. LR=0.000100\n",
            "0830 12:54:06 PM Epoch[004/050], Step[0000/0079], Avg Loss: 0.0095, Avg Acc: 0.7656\n",
            "0830 12:54:08 PM Epoch[004/050], Step[0020/0079], Avg Loss: 0.0121, Avg Acc: 0.7016\n",
            "0830 12:54:11 PM Epoch[004/050], Step[0040/0079], Avg Loss: 0.0120, Avg Acc: 0.7077\n",
            "0830 12:54:13 PM Epoch[004/050], Step[0060/0079], Avg Loss: 0.0120, Avg Acc: 0.7190\n",
            "0830 12:54:15 PM ----- Epoch[004/050], Train Loss: 0.0121, Train Acc: 0.7154, time: 8.47\n",
            "0830 12:54:15 PM Now training epoch 5. LR=0.000100\n",
            "0830 12:54:15 PM Epoch[005/050], Step[0000/0079], Avg Loss: 0.0062, Avg Acc: 0.8594\n",
            "0830 12:54:17 PM Epoch[005/050], Step[0020/0079], Avg Loss: 0.0119, Avg Acc: 0.7269\n",
            "0830 12:54:19 PM Epoch[005/050], Step[0040/0079], Avg Loss: 0.0114, Avg Acc: 0.7336\n",
            "0830 12:54:21 PM Epoch[005/050], Step[0060/0079], Avg Loss: 0.0114, Avg Acc: 0.7362\n",
            "0830 12:54:23 PM ----- Epoch[005/050], Train Loss: 0.0114, Train Acc: 0.7398, time: 8.54\n",
            "0830 12:54:23 PM Now training epoch 6. LR=0.000100\n",
            "0830 12:54:23 PM Epoch[006/050], Step[0000/0079], Avg Loss: 0.0072, Avg Acc: 0.8125\n",
            "0830 12:54:26 PM Epoch[006/050], Step[0020/0079], Avg Loss: 0.0109, Avg Acc: 0.7522\n",
            "0830 12:54:28 PM Epoch[006/050], Step[0040/0079], Avg Loss: 0.0110, Avg Acc: 0.7553\n",
            "0830 12:54:30 PM Epoch[006/050], Step[0060/0079], Avg Loss: 0.0108, Avg Acc: 0.7602\n",
            "0830 12:54:32 PM ----- Epoch[006/050], Train Loss: 0.0110, Train Acc: 0.7610, time: 8.56\n",
            "0830 12:54:32 PM Now training epoch 7. LR=0.000100\n",
            "0830 12:54:32 PM Epoch[007/050], Step[0000/0079], Avg Loss: 0.0084, Avg Acc: 0.8125\n",
            "0830 12:54:34 PM Epoch[007/050], Step[0020/0079], Avg Loss: 0.0101, Avg Acc: 0.7746\n",
            "0830 12:54:36 PM Epoch[007/050], Step[0040/0079], Avg Loss: 0.0098, Avg Acc: 0.7713\n",
            "0830 12:54:38 PM Epoch[007/050], Step[0060/0079], Avg Loss: 0.0095, Avg Acc: 0.7777\n",
            "0830 12:54:40 PM ----- Epoch[007/050], Train Loss: 0.0099, Train Acc: 0.7748, time: 8.60\n",
            "0830 12:54:40 PM Now training epoch 8. LR=0.000100\n",
            "0830 12:54:41 PM Epoch[008/050], Step[0000/0079], Avg Loss: 0.0088, Avg Acc: 0.7500\n",
            "0830 12:54:43 PM Epoch[008/050], Step[0020/0079], Avg Loss: 0.0094, Avg Acc: 0.7961\n",
            "0830 12:54:45 PM Epoch[008/050], Step[0040/0079], Avg Loss: 0.0098, Avg Acc: 0.7870\n",
            "0830 12:54:47 PM Epoch[008/050], Step[0060/0079], Avg Loss: 0.0096, Avg Acc: 0.7871\n",
            "0830 12:54:49 PM ----- Epoch[008/050], Train Loss: 0.0098, Train Acc: 0.7872, time: 8.65\n",
            "0830 12:54:49 PM Now training epoch 9. LR=0.000100\n",
            "0830 12:54:49 PM Epoch[009/050], Step[0000/0079], Avg Loss: 0.0074, Avg Acc: 0.8125\n",
            "0830 12:54:51 PM Epoch[009/050], Step[0020/0079], Avg Loss: 0.0091, Avg Acc: 0.7842\n",
            "0830 12:54:54 PM Epoch[009/050], Step[0040/0079], Avg Loss: 0.0089, Avg Acc: 0.7904\n",
            "0830 12:54:56 PM Epoch[009/050], Step[0060/0079], Avg Loss: 0.0087, Avg Acc: 0.7987\n",
            "0830 12:54:58 PM ----- Epoch[009/050], Train Loss: 0.0087, Train Acc: 0.8030, time: 8.64\n",
            "0830 12:54:58 PM Now training epoch 10. LR=0.000100\n",
            "0830 12:54:58 PM Epoch[010/050], Step[0000/0079], Avg Loss: 0.0088, Avg Acc: 0.8281\n",
            "0830 12:55:00 PM Epoch[010/050], Step[0020/0079], Avg Loss: 0.0083, Avg Acc: 0.8162\n",
            "0830 12:55:02 PM Epoch[010/050], Step[0040/0079], Avg Loss: 0.0083, Avg Acc: 0.8121\n",
            "0830 12:55:04 PM Epoch[010/050], Step[0060/0079], Avg Loss: 0.0082, Avg Acc: 0.8135\n",
            "0830 12:55:06 PM ----- Epoch[010/050], Train Loss: 0.0083, Train Acc: 0.8132, time: 8.61\n",
            "0830 12:55:06 PM Now training epoch 11. LR=0.000100\n",
            "0830 12:55:06 PM Epoch[011/050], Step[0000/0079], Avg Loss: 0.0076, Avg Acc: 0.8438\n",
            "0830 12:55:09 PM Epoch[011/050], Step[0020/0079], Avg Loss: 0.0080, Avg Acc: 0.8207\n",
            "0830 12:55:11 PM Epoch[011/050], Step[0040/0079], Avg Loss: 0.0083, Avg Acc: 0.8152\n",
            "0830 12:55:13 PM Epoch[011/050], Step[0060/0079], Avg Loss: 0.0079, Avg Acc: 0.8240\n",
            "0830 12:55:15 PM ----- Epoch[011/050], Train Loss: 0.0081, Train Acc: 0.8208, time: 8.56\n",
            "0830 12:55:15 PM Now training epoch 12. LR=0.000100\n",
            "0830 12:55:15 PM Epoch[012/050], Step[0000/0079], Avg Loss: 0.0062, Avg Acc: 0.8281\n",
            "0830 12:55:17 PM Epoch[012/050], Step[0020/0079], Avg Loss: 0.0085, Avg Acc: 0.7924\n",
            "0830 12:55:19 PM Epoch[012/050], Step[0040/0079], Avg Loss: 0.0084, Avg Acc: 0.8018\n",
            "0830 12:55:21 PM Epoch[012/050], Step[0060/0079], Avg Loss: 0.0080, Avg Acc: 0.8156\n",
            "0830 12:55:23 PM ----- Epoch[012/050], Train Loss: 0.0078, Train Acc: 0.8190, time: 8.57\n",
            "0830 12:55:23 PM Now training epoch 13. LR=0.000100\n",
            "0830 12:55:24 PM Epoch[013/050], Step[0000/0079], Avg Loss: 0.0062, Avg Acc: 0.8906\n",
            "0830 12:55:26 PM Epoch[013/050], Step[0020/0079], Avg Loss: 0.0068, Avg Acc: 0.8430\n",
            "0830 12:55:28 PM Epoch[013/050], Step[0040/0079], Avg Loss: 0.0071, Avg Acc: 0.8369\n",
            "0830 12:55:30 PM Epoch[013/050], Step[0060/0079], Avg Loss: 0.0072, Avg Acc: 0.8363\n",
            "0830 12:55:32 PM ----- Epoch[013/050], Train Loss: 0.0075, Train Acc: 0.8334, time: 8.70\n",
            "0830 12:55:32 PM Now training epoch 14. LR=0.000100\n",
            "0830 12:55:32 PM Epoch[014/050], Step[0000/0079], Avg Loss: 0.0078, Avg Acc: 0.7969\n",
            "0830 12:55:34 PM Epoch[014/050], Step[0020/0079], Avg Loss: 0.0066, Avg Acc: 0.8460\n",
            "0830 12:55:37 PM Epoch[014/050], Step[0040/0079], Avg Loss: 0.0069, Avg Acc: 0.8365\n",
            "0830 12:55:39 PM Epoch[014/050], Step[0060/0079], Avg Loss: 0.0068, Avg Acc: 0.8438\n",
            "0830 12:55:41 PM ----- Epoch[014/050], Train Loss: 0.0071, Train Acc: 0.8414, time: 8.57\n",
            "0830 12:55:41 PM Now training epoch 15. LR=0.000100\n",
            "0830 12:55:41 PM Epoch[015/050], Step[0000/0079], Avg Loss: 0.0057, Avg Acc: 0.8438\n",
            "0830 12:55:43 PM Epoch[015/050], Step[0020/0079], Avg Loss: 0.0072, Avg Acc: 0.8371\n",
            "0830 12:55:45 PM Epoch[015/050], Step[0040/0079], Avg Loss: 0.0070, Avg Acc: 0.8464\n",
            "0830 12:55:47 PM Epoch[015/050], Step[0060/0079], Avg Loss: 0.0070, Avg Acc: 0.8435\n",
            "0830 12:55:49 PM ----- Epoch[015/050], Train Loss: 0.0072, Train Acc: 0.8454, time: 8.55\n",
            "0830 12:55:49 PM Now training epoch 16. LR=0.000100\n",
            "0830 12:55:49 PM Epoch[016/050], Step[0000/0079], Avg Loss: 0.0064, Avg Acc: 0.8750\n",
            "0830 12:55:52 PM Epoch[016/050], Step[0020/0079], Avg Loss: 0.0081, Avg Acc: 0.8177\n",
            "0830 12:55:54 PM Epoch[016/050], Step[0040/0079], Avg Loss: 0.0077, Avg Acc: 0.8293\n",
            "0830 12:55:56 PM Epoch[016/050], Step[0060/0079], Avg Loss: 0.0072, Avg Acc: 0.8422\n",
            "0830 12:55:58 PM ----- Epoch[016/050], Train Loss: 0.0070, Train Acc: 0.8474, time: 8.56\n",
            "0830 12:55:58 PM Now training epoch 17. LR=0.000100\n",
            "0830 12:55:58 PM Epoch[017/050], Step[0000/0079], Avg Loss: 0.0053, Avg Acc: 0.9219\n",
            "0830 12:56:00 PM Epoch[017/050], Step[0020/0079], Avg Loss: 0.0059, Avg Acc: 0.8624\n",
            "0830 12:56:02 PM Epoch[017/050], Step[0040/0079], Avg Loss: 0.0060, Avg Acc: 0.8659\n",
            "0830 12:56:04 PM Epoch[017/050], Step[0060/0079], Avg Loss: 0.0061, Avg Acc: 0.8650\n",
            "0830 12:56:06 PM ----- Epoch[017/050], Train Loss: 0.0062, Train Acc: 0.8622, time: 8.61\n",
            "0830 12:56:06 PM Now training epoch 18. LR=0.000100\n",
            "0830 12:56:07 PM Epoch[018/050], Step[0000/0079], Avg Loss: 0.0045, Avg Acc: 0.9219\n",
            "0830 12:56:09 PM Epoch[018/050], Step[0020/0079], Avg Loss: 0.0067, Avg Acc: 0.8460\n",
            "0830 12:56:11 PM Epoch[018/050], Step[0040/0079], Avg Loss: 0.0061, Avg Acc: 0.8579\n",
            "0830 12:56:13 PM Epoch[018/050], Step[0060/0079], Avg Loss: 0.0058, Avg Acc: 0.8668\n",
            "0830 12:56:15 PM ----- Epoch[018/050], Train Loss: 0.0058, Train Acc: 0.8674, time: 8.60\n",
            "0830 12:56:15 PM Now training epoch 19. LR=0.000100\n",
            "0830 12:56:15 PM Epoch[019/050], Step[0000/0079], Avg Loss: 0.0067, Avg Acc: 0.8750\n",
            "0830 12:56:17 PM Epoch[019/050], Step[0020/0079], Avg Loss: 0.0049, Avg Acc: 0.8981\n",
            "0830 12:56:19 PM Epoch[019/050], Step[0040/0079], Avg Loss: 0.0051, Avg Acc: 0.8906\n",
            "0830 12:56:22 PM Epoch[019/050], Step[0060/0079], Avg Loss: 0.0054, Avg Acc: 0.8801\n",
            "0830 12:56:24 PM ----- Epoch[019/050], Train Loss: 0.0055, Train Acc: 0.8770, time: 8.59\n",
            "0830 12:56:24 PM Now training epoch 20. LR=0.000100\n",
            "0830 12:56:24 PM Epoch[020/050], Step[0000/0079], Avg Loss: 0.0056, Avg Acc: 0.8438\n",
            "0830 12:56:26 PM Epoch[020/050], Step[0020/0079], Avg Loss: 0.0051, Avg Acc: 0.8787\n",
            "0830 12:56:28 PM Epoch[020/050], Step[0040/0079], Avg Loss: 0.0049, Avg Acc: 0.8883\n",
            "0830 12:56:30 PM Epoch[020/050], Step[0060/0079], Avg Loss: 0.0053, Avg Acc: 0.8858\n",
            "0830 12:56:32 PM ----- Epoch[020/050], Train Loss: 0.0052, Train Acc: 0.8884, time: 8.60\n",
            "0830 12:56:32 PM Now training epoch 21. LR=0.000100\n",
            "0830 12:56:32 PM Epoch[021/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9219\n",
            "0830 12:56:35 PM Epoch[021/050], Step[0020/0079], Avg Loss: 0.0054, Avg Acc: 0.8765\n",
            "0830 12:56:37 PM Epoch[021/050], Step[0040/0079], Avg Loss: 0.0050, Avg Acc: 0.8807\n",
            "0830 12:56:39 PM Epoch[021/050], Step[0060/0079], Avg Loss: 0.0051, Avg Acc: 0.8814\n",
            "0830 12:56:41 PM ----- Epoch[021/050], Train Loss: 0.0051, Train Acc: 0.8850, time: 8.61\n",
            "0830 12:56:41 PM Now training epoch 22. LR=0.000100\n",
            "0830 12:56:41 PM Epoch[022/050], Step[0000/0079], Avg Loss: 0.0049, Avg Acc: 0.9219\n",
            "0830 12:56:43 PM Epoch[022/050], Step[0020/0079], Avg Loss: 0.0052, Avg Acc: 0.8847\n",
            "0830 12:56:45 PM Epoch[022/050], Step[0040/0079], Avg Loss: 0.0054, Avg Acc: 0.8815\n",
            "0830 12:56:47 PM Epoch[022/050], Step[0060/0079], Avg Loss: 0.0053, Avg Acc: 0.8827\n",
            "0830 12:56:49 PM ----- Epoch[022/050], Train Loss: 0.0054, Train Acc: 0.8872, time: 8.56\n",
            "0830 12:56:49 PM Now training epoch 23. LR=0.000100\n",
            "0830 12:56:50 PM Epoch[023/050], Step[0000/0079], Avg Loss: 0.0048, Avg Acc: 0.9062\n",
            "0830 12:56:52 PM Epoch[023/050], Step[0020/0079], Avg Loss: 0.0073, Avg Acc: 0.8304\n",
            "0830 12:56:54 PM Epoch[023/050], Step[0040/0079], Avg Loss: 0.0062, Avg Acc: 0.8639\n",
            "0830 12:56:56 PM Epoch[023/050], Step[0060/0079], Avg Loss: 0.0059, Avg Acc: 0.8717\n",
            "0830 12:56:58 PM ----- Epoch[023/050], Train Loss: 0.0058, Train Acc: 0.8758, time: 8.57\n",
            "0830 12:56:58 PM Now training epoch 24. LR=0.000100\n",
            "0830 12:56:58 PM Epoch[024/050], Step[0000/0079], Avg Loss: 0.0028, Avg Acc: 0.9219\n",
            "0830 12:57:00 PM Epoch[024/050], Step[0020/0079], Avg Loss: 0.0045, Avg Acc: 0.8981\n",
            "0830 12:57:02 PM Epoch[024/050], Step[0040/0079], Avg Loss: 0.0045, Avg Acc: 0.8990\n",
            "0830 12:57:05 PM Epoch[024/050], Step[0060/0079], Avg Loss: 0.0043, Avg Acc: 0.9037\n",
            "0830 12:57:06 PM ----- Epoch[024/050], Train Loss: 0.0043, Train Acc: 0.9058, time: 8.57\n",
            "0830 12:57:06 PM Now training epoch 25. LR=0.000100\n",
            "0830 12:57:07 PM Epoch[025/050], Step[0000/0079], Avg Loss: 0.0034, Avg Acc: 0.9219\n",
            "0830 12:57:09 PM Epoch[025/050], Step[0020/0079], Avg Loss: 0.0036, Avg Acc: 0.9293\n",
            "0830 12:57:11 PM Epoch[025/050], Step[0040/0079], Avg Loss: 0.0039, Avg Acc: 0.9192\n",
            "0830 12:57:13 PM Epoch[025/050], Step[0060/0079], Avg Loss: 0.0040, Avg Acc: 0.9144\n",
            "0830 12:57:15 PM ----- Epoch[025/050], Train Loss: 0.0041, Train Acc: 0.9128, time: 8.57\n",
            "0830 12:57:19 PM ----- Save model: ./outputs2/train-20220830-12-53-37/Epoch-25-Loss-0.004111099776625634.pth\n",
            "0830 12:57:19 PM Now training epoch 26. LR=0.000100\n",
            "0830 12:57:20 PM Epoch[026/050], Step[0000/0079], Avg Loss: 0.0039, Avg Acc: 0.9375\n",
            "0830 12:57:22 PM Epoch[026/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9077\n",
            "0830 12:57:24 PM Epoch[026/050], Step[0040/0079], Avg Loss: 0.0042, Avg Acc: 0.9085\n",
            "0830 12:57:26 PM Epoch[026/050], Step[0060/0079], Avg Loss: 0.0040, Avg Acc: 0.9132\n",
            "0830 12:57:28 PM ----- Epoch[026/050], Train Loss: 0.0043, Train Acc: 0.9088, time: 8.70\n",
            "0830 12:57:28 PM Now training epoch 27. LR=0.000100\n",
            "0830 12:57:28 PM Epoch[027/050], Step[0000/0079], Avg Loss: 0.0037, Avg Acc: 0.9062\n",
            "0830 12:57:31 PM Epoch[027/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9092\n",
            "0830 12:57:33 PM Epoch[027/050], Step[0040/0079], Avg Loss: 0.0047, Avg Acc: 0.9028\n",
            "0830 12:57:35 PM Epoch[027/050], Step[0060/0079], Avg Loss: 0.0045, Avg Acc: 0.9027\n",
            "0830 12:57:37 PM ----- Epoch[027/050], Train Loss: 0.0044, Train Acc: 0.9046, time: 8.59\n",
            "0830 12:57:37 PM Now training epoch 28. LR=0.000100\n",
            "0830 12:57:37 PM Epoch[028/050], Step[0000/0079], Avg Loss: 0.0021, Avg Acc: 0.9531\n",
            "0830 12:57:39 PM Epoch[028/050], Step[0020/0079], Avg Loss: 0.0035, Avg Acc: 0.9241\n",
            "0830 12:57:41 PM Epoch[028/050], Step[0040/0079], Avg Loss: 0.0034, Avg Acc: 0.9284\n",
            "0830 12:57:44 PM Epoch[028/050], Step[0060/0079], Avg Loss: 0.0034, Avg Acc: 0.9262\n",
            "0830 12:57:46 PM ----- Epoch[028/050], Train Loss: 0.0038, Train Acc: 0.9200, time: 8.73\n",
            "0830 12:57:46 PM Now training epoch 29. LR=0.000100\n",
            "0830 12:57:46 PM Epoch[029/050], Step[0000/0079], Avg Loss: 0.0037, Avg Acc: 0.9375\n",
            "0830 12:57:48 PM Epoch[029/050], Step[0020/0079], Avg Loss: 0.0042, Avg Acc: 0.9196\n",
            "0830 12:57:50 PM Epoch[029/050], Step[0040/0079], Avg Loss: 0.0041, Avg Acc: 0.9196\n",
            "0830 12:57:52 PM Epoch[029/050], Step[0060/0079], Avg Loss: 0.0042, Avg Acc: 0.9162\n",
            "0830 12:57:54 PM ----- Epoch[029/050], Train Loss: 0.0040, Train Acc: 0.9192, time: 8.62\n",
            "0830 12:57:54 PM Now training epoch 30. LR=0.000100\n",
            "0830 12:57:54 PM Epoch[030/050], Step[0000/0079], Avg Loss: 0.0035, Avg Acc: 0.9062\n",
            "0830 12:57:57 PM Epoch[030/050], Step[0020/0079], Avg Loss: 0.0039, Avg Acc: 0.9137\n",
            "0830 12:57:59 PM Epoch[030/050], Step[0040/0079], Avg Loss: 0.0036, Avg Acc: 0.9177\n",
            "0830 12:58:01 PM Epoch[030/050], Step[0060/0079], Avg Loss: 0.0034, Avg Acc: 0.9247\n",
            "0830 12:58:03 PM ----- Epoch[030/050], Train Loss: 0.0035, Train Acc: 0.9230, time: 8.62\n",
            "0830 12:58:03 PM Now training epoch 31. LR=0.000100\n",
            "0830 12:58:03 PM Epoch[031/050], Step[0000/0079], Avg Loss: 0.0055, Avg Acc: 0.8438\n",
            "0830 12:58:05 PM Epoch[031/050], Step[0020/0079], Avg Loss: 0.0043, Avg Acc: 0.9040\n",
            "0830 12:58:07 PM Epoch[031/050], Step[0040/0079], Avg Loss: 0.0039, Avg Acc: 0.9169\n",
            "0830 12:58:09 PM Epoch[031/050], Step[0060/0079], Avg Loss: 0.0039, Avg Acc: 0.9168\n",
            "0830 12:58:11 PM ----- Epoch[031/050], Train Loss: 0.0040, Train Acc: 0.9122, time: 8.61\n",
            "0830 12:58:11 PM Now training epoch 32. LR=0.000100\n",
            "0830 12:58:12 PM Epoch[032/050], Step[0000/0079], Avg Loss: 0.0024, Avg Acc: 0.9375\n",
            "0830 12:58:14 PM Epoch[032/050], Step[0020/0079], Avg Loss: 0.0032, Avg Acc: 0.9256\n",
            "0830 12:58:16 PM Epoch[032/050], Step[0040/0079], Avg Loss: 0.0034, Avg Acc: 0.9249\n",
            "0830 12:58:18 PM Epoch[032/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9283\n",
            "0830 12:58:20 PM ----- Epoch[032/050], Train Loss: 0.0033, Train Acc: 0.9280, time: 8.58\n",
            "0830 12:58:20 PM Now training epoch 33. LR=0.000100\n",
            "0830 12:58:20 PM Epoch[033/050], Step[0000/0079], Avg Loss: 0.0016, Avg Acc: 0.9688\n",
            "0830 12:58:22 PM Epoch[033/050], Step[0020/0079], Avg Loss: 0.0032, Avg Acc: 0.9405\n",
            "0830 12:58:24 PM Epoch[033/050], Step[0040/0079], Avg Loss: 0.0031, Avg Acc: 0.9379\n",
            "0830 12:58:27 PM Epoch[033/050], Step[0060/0079], Avg Loss: 0.0030, Avg Acc: 0.9372\n",
            "0830 12:58:29 PM ----- Epoch[033/050], Train Loss: 0.0029, Train Acc: 0.9358, time: 8.61\n",
            "0830 12:58:29 PM Now training epoch 34. LR=0.000100\n",
            "0830 12:58:29 PM Epoch[034/050], Step[0000/0079], Avg Loss: 0.0017, Avg Acc: 0.9688\n",
            "0830 12:58:31 PM Epoch[034/050], Step[0020/0079], Avg Loss: 0.0030, Avg Acc: 0.9338\n",
            "0830 12:58:33 PM Epoch[034/050], Step[0040/0079], Avg Loss: 0.0033, Avg Acc: 0.9238\n",
            "0830 12:58:35 PM Epoch[034/050], Step[0060/0079], Avg Loss: 0.0032, Avg Acc: 0.9283\n",
            "0830 12:58:37 PM ----- Epoch[034/050], Train Loss: 0.0031, Train Acc: 0.9306, time: 8.58\n",
            "0830 12:58:37 PM Now training epoch 35. LR=0.000100\n",
            "0830 12:58:37 PM Epoch[035/050], Step[0000/0079], Avg Loss: 0.0024, Avg Acc: 0.9375\n",
            "0830 12:58:40 PM Epoch[035/050], Step[0020/0079], Avg Loss: 0.0030, Avg Acc: 0.9368\n",
            "0830 12:58:42 PM Epoch[035/050], Step[0040/0079], Avg Loss: 0.0029, Avg Acc: 0.9394\n",
            "0830 12:58:44 PM Epoch[035/050], Step[0060/0079], Avg Loss: 0.0031, Avg Acc: 0.9367\n",
            "0830 12:58:46 PM ----- Epoch[035/050], Train Loss: 0.0036, Train Acc: 0.9282, time: 8.59\n",
            "0830 12:58:46 PM Now training epoch 36. LR=0.000100\n",
            "0830 12:58:46 PM Epoch[036/050], Step[0000/0079], Avg Loss: 0.0014, Avg Acc: 0.9844\n",
            "0830 12:58:48 PM Epoch[036/050], Step[0020/0079], Avg Loss: 0.0041, Avg Acc: 0.9204\n",
            "0830 12:58:50 PM Epoch[036/050], Step[0040/0079], Avg Loss: 0.0036, Avg Acc: 0.9268\n",
            "0830 12:58:52 PM Epoch[036/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9306\n",
            "0830 12:58:54 PM ----- Epoch[036/050], Train Loss: 0.0034, Train Acc: 0.9294, time: 8.59\n",
            "0830 12:58:54 PM Now training epoch 37. LR=0.000100\n",
            "0830 12:58:55 PM Epoch[037/050], Step[0000/0079], Avg Loss: 0.0026, Avg Acc: 0.9844\n",
            "0830 12:58:57 PM Epoch[037/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9412\n",
            "0830 12:58:59 PM Epoch[037/050], Step[0040/0079], Avg Loss: 0.0030, Avg Acc: 0.9364\n",
            "0830 12:59:01 PM Epoch[037/050], Step[0060/0079], Avg Loss: 0.0029, Avg Acc: 0.9383\n",
            "0830 12:59:03 PM ----- Epoch[037/050], Train Loss: 0.0029, Train Acc: 0.9402, time: 8.60\n",
            "0830 12:59:03 PM Now training epoch 38. LR=0.000100\n",
            "0830 12:59:03 PM Epoch[038/050], Step[0000/0079], Avg Loss: 0.0029, Avg Acc: 0.9219\n",
            "0830 12:59:05 PM Epoch[038/050], Step[0020/0079], Avg Loss: 0.0035, Avg Acc: 0.9315\n",
            "0830 12:59:07 PM Epoch[038/050], Step[0040/0079], Avg Loss: 0.0035, Avg Acc: 0.9280\n",
            "0830 12:59:10 PM Epoch[038/050], Step[0060/0079], Avg Loss: 0.0033, Avg Acc: 0.9329\n",
            "0830 12:59:12 PM ----- Epoch[038/050], Train Loss: 0.0031, Train Acc: 0.9384, time: 8.61\n",
            "0830 12:59:12 PM Now training epoch 39. LR=0.000100\n",
            "0830 12:59:12 PM Epoch[039/050], Step[0000/0079], Avg Loss: 0.0018, Avg Acc: 0.9688\n",
            "0830 12:59:14 PM Epoch[039/050], Step[0020/0079], Avg Loss: 0.0028, Avg Acc: 0.9323\n",
            "0830 12:59:16 PM Epoch[039/050], Step[0040/0079], Avg Loss: 0.0029, Avg Acc: 0.9318\n",
            "0830 12:59:18 PM Epoch[039/050], Step[0060/0079], Avg Loss: 0.0029, Avg Acc: 0.9342\n",
            "0830 12:59:20 PM ----- Epoch[039/050], Train Loss: 0.0030, Train Acc: 0.9340, time: 8.58\n",
            "0830 12:59:20 PM Now training epoch 40. LR=0.000100\n",
            "0830 12:59:20 PM Epoch[040/050], Step[0000/0079], Avg Loss: 0.0033, Avg Acc: 0.9375\n",
            "0830 12:59:22 PM Epoch[040/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9583\n",
            "0830 12:59:25 PM Epoch[040/050], Step[0040/0079], Avg Loss: 0.0025, Avg Acc: 0.9482\n",
            "0830 12:59:27 PM Epoch[040/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9413\n",
            "0830 12:59:29 PM ----- Epoch[040/050], Train Loss: 0.0029, Train Acc: 0.9418, time: 8.60\n",
            "0830 12:59:29 PM Now training epoch 41. LR=0.000100\n",
            "0830 12:59:29 PM Epoch[041/050], Step[0000/0079], Avg Loss: 0.0040, Avg Acc: 0.9531\n",
            "0830 12:59:31 PM Epoch[041/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9554\n",
            "0830 12:59:33 PM Epoch[041/050], Step[0040/0079], Avg Loss: 0.0025, Avg Acc: 0.9520\n",
            "0830 12:59:35 PM Epoch[041/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9493\n",
            "0830 12:59:37 PM ----- Epoch[041/050], Train Loss: 0.0024, Train Acc: 0.9514, time: 8.59\n",
            "0830 12:59:37 PM Now training epoch 42. LR=0.000100\n",
            "0830 12:59:38 PM Epoch[042/050], Step[0000/0079], Avg Loss: 0.0031, Avg Acc: 0.9375\n",
            "0830 12:59:40 PM Epoch[042/050], Step[0020/0079], Avg Loss: 0.0022, Avg Acc: 0.9576\n",
            "0830 12:59:42 PM Epoch[042/050], Step[0040/0079], Avg Loss: 0.0025, Avg Acc: 0.9516\n",
            "0830 12:59:44 PM Epoch[042/050], Step[0060/0079], Avg Loss: 0.0027, Avg Acc: 0.9454\n",
            "0830 12:59:46 PM ----- Epoch[042/050], Train Loss: 0.0027, Train Acc: 0.9462, time: 8.58\n",
            "0830 12:59:46 PM Now training epoch 43. LR=0.000100\n",
            "0830 12:59:46 PM Epoch[043/050], Step[0000/0079], Avg Loss: 0.0033, Avg Acc: 0.9531\n",
            "0830 12:59:48 PM Epoch[043/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9561\n",
            "0830 12:59:50 PM Epoch[043/050], Step[0040/0079], Avg Loss: 0.0020, Avg Acc: 0.9596\n",
            "0830 12:59:53 PM Epoch[043/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9608\n",
            "0830 12:59:54 PM ----- Epoch[043/050], Train Loss: 0.0021, Train Acc: 0.9612, time: 8.58\n",
            "0830 12:59:54 PM Now training epoch 44. LR=0.000100\n",
            "0830 12:59:55 PM Epoch[044/050], Step[0000/0079], Avg Loss: 0.0041, Avg Acc: 0.9219\n",
            "0830 12:59:57 PM Epoch[044/050], Step[0020/0079], Avg Loss: 0.0051, Avg Acc: 0.9100\n",
            "0830 12:59:59 PM Epoch[044/050], Step[0040/0079], Avg Loss: 0.0042, Avg Acc: 0.9230\n",
            "0830 01:00:01 PM Epoch[044/050], Step[0060/0079], Avg Loss: 0.0038, Avg Acc: 0.9265\n",
            "0830 01:00:03 PM ----- Epoch[044/050], Train Loss: 0.0035, Train Acc: 0.9308, time: 8.73\n",
            "0830 01:00:03 PM Now training epoch 45. LR=0.000100\n",
            "0830 01:00:03 PM Epoch[045/050], Step[0000/0079], Avg Loss: 0.0023, Avg Acc: 0.9375\n",
            "0830 01:00:06 PM Epoch[045/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9516\n",
            "0830 01:00:08 PM Epoch[045/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9562\n",
            "0830 01:00:10 PM Epoch[045/050], Step[0060/0079], Avg Loss: 0.0022, Avg Acc: 0.9557\n",
            "0830 01:00:12 PM ----- Epoch[045/050], Train Loss: 0.0025, Train Acc: 0.9518, time: 8.58\n",
            "0830 01:00:12 PM Now training epoch 46. LR=0.000100\n",
            "0830 01:00:12 PM Epoch[046/050], Step[0000/0079], Avg Loss: 0.0034, Avg Acc: 0.9375\n",
            "0830 01:00:14 PM Epoch[046/050], Step[0020/0079], Avg Loss: 0.0022, Avg Acc: 0.9539\n",
            "0830 01:00:16 PM Epoch[046/050], Step[0040/0079], Avg Loss: 0.0021, Avg Acc: 0.9535\n",
            "0830 01:00:18 PM Epoch[046/050], Step[0060/0079], Avg Loss: 0.0022, Avg Acc: 0.9524\n",
            "0830 01:00:20 PM ----- Epoch[046/050], Train Loss: 0.0022, Train Acc: 0.9548, time: 8.55\n",
            "0830 01:00:20 PM Now training epoch 47. LR=0.000100\n",
            "0830 01:00:21 PM Epoch[047/050], Step[0000/0079], Avg Loss: 0.0041, Avg Acc: 0.9219\n",
            "0830 01:00:23 PM Epoch[047/050], Step[0020/0079], Avg Loss: 0.0051, Avg Acc: 0.9048\n",
            "0830 01:00:25 PM Epoch[047/050], Step[0040/0079], Avg Loss: 0.0042, Avg Acc: 0.9192\n",
            "0830 01:00:27 PM Epoch[047/050], Step[0060/0079], Avg Loss: 0.0039, Avg Acc: 0.9229\n",
            "0830 01:00:29 PM ----- Epoch[047/050], Train Loss: 0.0037, Train Acc: 0.9300, time: 8.57\n",
            "0830 01:00:29 PM Now training epoch 48. LR=0.000100\n",
            "0830 01:00:29 PM Epoch[048/050], Step[0000/0079], Avg Loss: 0.0023, Avg Acc: 0.9531\n",
            "0830 01:00:31 PM Epoch[048/050], Step[0020/0079], Avg Loss: 0.0026, Avg Acc: 0.9427\n",
            "0830 01:00:33 PM Epoch[048/050], Step[0040/0079], Avg Loss: 0.0024, Avg Acc: 0.9466\n",
            "0830 01:00:36 PM Epoch[048/050], Step[0060/0079], Avg Loss: 0.0023, Avg Acc: 0.9503\n",
            "0830 01:00:37 PM ----- Epoch[048/050], Train Loss: 0.0023, Train Acc: 0.9500, time: 8.57\n",
            "0830 01:00:37 PM Now training epoch 49. LR=0.000100\n",
            "0830 01:00:38 PM Epoch[049/050], Step[0000/0079], Avg Loss: 0.0013, Avg Acc: 0.9531\n",
            "0830 01:00:40 PM Epoch[049/050], Step[0020/0079], Avg Loss: 0.0017, Avg Acc: 0.9650\n",
            "0830 01:00:42 PM Epoch[049/050], Step[0040/0079], Avg Loss: 0.0018, Avg Acc: 0.9615\n",
            "0830 01:00:44 PM Epoch[049/050], Step[0060/0079], Avg Loss: 0.0019, Avg Acc: 0.9595\n",
            "0830 01:00:46 PM ----- Epoch[049/050], Train Loss: 0.0020, Train Acc: 0.9574, time: 8.59\n",
            "0830 01:00:46 PM Now training epoch 50. LR=0.000100\n",
            "0830 01:00:46 PM Epoch[050/050], Step[0000/0079], Avg Loss: 0.0030, Avg Acc: 0.8906\n",
            "0830 01:00:48 PM Epoch[050/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9487\n",
            "0830 01:00:51 PM Epoch[050/050], Step[0040/0079], Avg Loss: 0.0021, Avg Acc: 0.9539\n",
            "0830 01:00:53 PM Epoch[050/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9582\n",
            "0830 01:00:55 PM ----- Epoch[050/050], Train Loss: 0.0021, Train Acc: 0.9570, time: 8.60\n",
            "0830 01:00:59 PM ----- Save model: ./outputs2/train-20220830-12-53-37/Epoch-50-Loss-0.0021423590056598185.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --mode=test --batch_size=32 --output=\"./outputs2/\" --problem=2"
      ],
      "metadata": {
        "id": "hH5W5hkkyJPM",
        "outputId": "628e2d6e-da29-49b8-8d9c-e775db880cbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "0830 01:01:10 PM ----- Start Test\n",
            "0830 01:01:15 PM Test Step[0000/0032], Avg Loss: 0.0133, Avg Test Acc: 0.8750\n",
            "0830 01:01:15 PM Test Step[0010/0032], Avg Loss: 0.0180, Avg Test Acc: 0.8665\n",
            "0830 01:01:15 PM Test Step[0020/0032], Avg Loss: 0.0225, Avg Test Acc: 0.8393\n",
            "0830 01:01:15 PM Test Step[0030/0032], Avg Loss: 0.0333, Avg Test Acc: 0.7812\n",
            "0830 01:01:15 PM Test Loss: 0.0344, Test Acc: 0.7810, time: 2.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --tsne --mode=test --batch_size=32 --output=\"./outputs1/\" --problem=1"
      ],
      "metadata": {
        "id": "CiZZlj0syLk5",
        "outputId": "05116e20-6eec-468f-ffbd-e9cead07fe54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
            "  FutureWarning,\n",
            "Finish T-SNE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --tsne --mode=test --batch_size=32 --output=\"./outputs2/\" --problem=2"
      ],
      "metadata": {
        "id": "ydq8kyzgygCA",
        "outputId": "f9567d32-fbe1-4910-9034-c73ace9c6619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
            "  FutureWarning,\n",
            "Finish T-SNE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwU782Qe2CrQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}