{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XUProblem.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNB+XHD2rOF03dp+nSc4zvz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wyxogo/XUProblem/blob/master/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wyxogo/XUProblem.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Z51AoWp7ZX",
        "outputId": "5cdaf4fd-5961-45ca-cdf6-eaea8eb64d16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'XUProblem'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 54 (delta 25), reused 38 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/XUProblem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpDA6ZYuajQe",
        "outputId": "66134a1d-740d-45e3-a597-b47005b005a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/XUProblem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --mode=train --batch_size=64 --epochs=50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxaHUTGl0AnY",
        "outputId": "42ba2f06-d936-4420-bebd-be9b35b6918f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "0829 07:08:40 AM Now training epoch 2. LR=0.000100\n",
            "0829 07:08:44 AM Epoch[001/050], Step[0000/0079], Avg Loss: 0.0767, Avg Acc: 0.1406\n",
            "0829 07:08:46 AM Epoch[001/050], Step[0020/0079], Avg Loss: 0.0321, Avg Acc: 0.3296\n",
            "0829 07:08:48 AM Epoch[001/050], Step[0040/0079], Avg Loss: 0.0255, Avg Acc: 0.4630\n",
            "0829 07:08:50 AM Epoch[001/050], Step[0060/0079], Avg Loss: 0.0220, Avg Acc: 0.5405\n",
            "0829 07:08:52 AM ----- Epoch[001/050], Train Loss: 0.0199, Train Acc: 0.5908, time: 10.41\n",
            "0829 07:08:52 AM Now training epoch 3. LR=0.000100\n",
            "0829 07:08:52 AM Epoch[002/050], Step[0000/0079], Avg Loss: 0.0092, Avg Acc: 0.8125\n",
            "0829 07:08:54 AM Epoch[002/050], Step[0020/0079], Avg Loss: 0.0098, Avg Acc: 0.8036\n",
            "0829 07:08:57 AM Epoch[002/050], Step[0040/0079], Avg Loss: 0.0098, Avg Acc: 0.8041\n",
            "0829 07:08:59 AM Epoch[002/050], Step[0060/0079], Avg Loss: 0.0097, Avg Acc: 0.8092\n",
            "0829 07:09:01 AM ----- Epoch[002/050], Train Loss: 0.0096, Train Acc: 0.8112, time: 8.37\n",
            "0829 07:09:01 AM Now training epoch 4. LR=0.000100\n",
            "0829 07:09:01 AM Epoch[003/050], Step[0000/0079], Avg Loss: 0.0066, Avg Acc: 0.8438\n",
            "0829 07:09:03 AM Epoch[003/050], Step[0020/0079], Avg Loss: 0.0073, Avg Acc: 0.8579\n",
            "0829 07:09:05 AM Epoch[003/050], Step[0040/0079], Avg Loss: 0.0083, Avg Acc: 0.8426\n",
            "0829 07:09:07 AM Epoch[003/050], Step[0060/0079], Avg Loss: 0.0082, Avg Acc: 0.8443\n",
            "0829 07:09:09 AM ----- Epoch[003/050], Train Loss: 0.0082, Train Acc: 0.8472, time: 8.40\n",
            "0829 07:09:09 AM Now training epoch 5. LR=0.000100\n",
            "0829 07:09:09 AM Epoch[004/050], Step[0000/0079], Avg Loss: 0.0071, Avg Acc: 0.8438\n",
            "0829 07:09:11 AM Epoch[004/050], Step[0020/0079], Avg Loss: 0.0073, Avg Acc: 0.8616\n",
            "0829 07:09:13 AM Epoch[004/050], Step[0040/0079], Avg Loss: 0.0072, Avg Acc: 0.8579\n",
            "0829 07:09:16 AM Epoch[004/050], Step[0060/0079], Avg Loss: 0.0073, Avg Acc: 0.8550\n",
            "0829 07:09:18 AM ----- Epoch[004/050], Train Loss: 0.0072, Train Acc: 0.8590, time: 8.57\n",
            "0829 07:09:18 AM Now training epoch 6. LR=0.000100\n",
            "0829 07:09:18 AM Epoch[005/050], Step[0000/0079], Avg Loss: 0.0050, Avg Acc: 0.9219\n",
            "0829 07:09:20 AM Epoch[005/050], Step[0020/0079], Avg Loss: 0.0059, Avg Acc: 0.8869\n",
            "0829 07:09:22 AM Epoch[005/050], Step[0040/0079], Avg Loss: 0.0054, Avg Acc: 0.8963\n",
            "0829 07:09:24 AM Epoch[005/050], Step[0060/0079], Avg Loss: 0.0056, Avg Acc: 0.8929\n",
            "0829 07:09:26 AM ----- Epoch[005/050], Train Loss: 0.0054, Train Acc: 0.8968, time: 8.45\n",
            "0829 07:09:26 AM Now training epoch 7. LR=0.000100\n",
            "0829 07:09:26 AM Epoch[006/050], Step[0000/0079], Avg Loss: 0.0032, Avg Acc: 0.9062\n",
            "0829 07:09:28 AM Epoch[006/050], Step[0020/0079], Avg Loss: 0.0052, Avg Acc: 0.9010\n",
            "0829 07:09:30 AM Epoch[006/050], Step[0040/0079], Avg Loss: 0.0053, Avg Acc: 0.8982\n",
            "0829 07:09:33 AM Epoch[006/050], Step[0060/0079], Avg Loss: 0.0052, Avg Acc: 0.8988\n",
            "0829 07:09:34 AM ----- Epoch[006/050], Train Loss: 0.0054, Train Acc: 0.8966, time: 8.49\n",
            "0829 07:09:34 AM Now training epoch 8. LR=0.000100\n",
            "0829 07:09:35 AM Epoch[007/050], Step[0000/0079], Avg Loss: 0.0072, Avg Acc: 0.8281\n",
            "0829 07:09:37 AM Epoch[007/050], Step[0020/0079], Avg Loss: 0.0056, Avg Acc: 0.8832\n",
            "0829 07:09:39 AM Epoch[007/050], Step[0040/0079], Avg Loss: 0.0053, Avg Acc: 0.8960\n",
            "0829 07:09:41 AM Epoch[007/050], Step[0060/0079], Avg Loss: 0.0054, Avg Acc: 0.8932\n",
            "0829 07:09:43 AM ----- Epoch[007/050], Train Loss: 0.0053, Train Acc: 0.8968, time: 8.52\n",
            "0829 07:09:43 AM Now training epoch 9. LR=0.000100\n",
            "0829 07:09:43 AM Epoch[008/050], Step[0000/0079], Avg Loss: 0.0031, Avg Acc: 0.9844\n",
            "0829 07:09:45 AM Epoch[008/050], Step[0020/0079], Avg Loss: 0.0047, Avg Acc: 0.9137\n",
            "0829 07:09:47 AM Epoch[008/050], Step[0040/0079], Avg Loss: 0.0043, Avg Acc: 0.9204\n",
            "0829 07:09:50 AM Epoch[008/050], Step[0060/0079], Avg Loss: 0.0042, Avg Acc: 0.9211\n",
            "0829 07:09:51 AM ----- Epoch[008/050], Train Loss: 0.0044, Train Acc: 0.9194, time: 8.52\n",
            "0829 07:09:51 AM Now training epoch 10. LR=0.000100\n",
            "0829 07:09:52 AM Epoch[009/050], Step[0000/0079], Avg Loss: 0.0015, Avg Acc: 0.9688\n",
            "0829 07:09:54 AM Epoch[009/050], Step[0020/0079], Avg Loss: 0.0036, Avg Acc: 0.9420\n",
            "0829 07:09:56 AM Epoch[009/050], Step[0040/0079], Avg Loss: 0.0036, Avg Acc: 0.9352\n",
            "0829 07:09:58 AM Epoch[009/050], Step[0060/0079], Avg Loss: 0.0038, Avg Acc: 0.9308\n",
            "0829 07:10:00 AM ----- Epoch[009/050], Train Loss: 0.0040, Train Acc: 0.9274, time: 8.58\n",
            "0829 07:10:00 AM Now training epoch 11. LR=0.000100\n",
            "0829 07:10:00 AM Epoch[010/050], Step[0000/0079], Avg Loss: 0.0030, Avg Acc: 0.9531\n",
            "0829 07:10:02 AM Epoch[010/050], Step[0020/0079], Avg Loss: 0.0037, Avg Acc: 0.9323\n",
            "0829 07:10:05 AM Epoch[010/050], Step[0040/0079], Avg Loss: 0.0034, Avg Acc: 0.9367\n",
            "0829 07:10:07 AM Epoch[010/050], Step[0060/0079], Avg Loss: 0.0036, Avg Acc: 0.9329\n",
            "0829 07:10:09 AM ----- Epoch[010/050], Train Loss: 0.0038, Train Acc: 0.9318, time: 8.60\n",
            "0829 07:10:09 AM Now training epoch 12. LR=0.000100\n",
            "0829 07:10:09 AM Epoch[011/050], Step[0000/0079], Avg Loss: 0.0038, Avg Acc: 0.9219\n",
            "0829 07:10:11 AM Epoch[011/050], Step[0020/0079], Avg Loss: 0.0048, Avg Acc: 0.9025\n",
            "0829 07:10:13 AM Epoch[011/050], Step[0040/0079], Avg Loss: 0.0038, Avg Acc: 0.9238\n",
            "0829 07:10:15 AM Epoch[011/050], Step[0060/0079], Avg Loss: 0.0038, Avg Acc: 0.9242\n",
            "0829 07:10:17 AM ----- Epoch[011/050], Train Loss: 0.0036, Train Acc: 0.9290, time: 8.64\n",
            "0829 07:10:17 AM Now training epoch 13. LR=0.000100\n",
            "0829 07:10:18 AM Epoch[012/050], Step[0000/0079], Avg Loss: 0.0025, Avg Acc: 0.9531\n",
            "0829 07:10:20 AM Epoch[012/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9621\n",
            "0829 07:10:22 AM Epoch[012/050], Step[0040/0079], Avg Loss: 0.0027, Avg Acc: 0.9516\n",
            "0829 07:10:24 AM Epoch[012/050], Step[0060/0079], Avg Loss: 0.0028, Avg Acc: 0.9472\n",
            "0829 07:10:26 AM ----- Epoch[012/050], Train Loss: 0.0028, Train Acc: 0.9476, time: 8.66\n",
            "0829 07:10:26 AM Now training epoch 14. LR=0.000100\n",
            "0829 07:10:26 AM Epoch[013/050], Step[0000/0079], Avg Loss: 0.0043, Avg Acc: 0.9062\n",
            "0829 07:10:28 AM Epoch[013/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9598\n",
            "0829 07:10:31 AM Epoch[013/050], Step[0040/0079], Avg Loss: 0.0028, Avg Acc: 0.9489\n",
            "0829 07:10:33 AM Epoch[013/050], Step[0060/0079], Avg Loss: 0.0029, Avg Acc: 0.9454\n",
            "0829 07:10:35 AM ----- Epoch[013/050], Train Loss: 0.0030, Train Acc: 0.9434, time: 8.66\n",
            "0829 07:10:35 AM Now training epoch 15. LR=0.000100\n",
            "0829 07:10:35 AM Epoch[014/050], Step[0000/0079], Avg Loss: 0.0059, Avg Acc: 0.9219\n",
            "0829 07:10:37 AM Epoch[014/050], Step[0020/0079], Avg Loss: 0.0025, Avg Acc: 0.9554\n",
            "0829 07:10:39 AM Epoch[014/050], Step[0040/0079], Avg Loss: 0.0029, Avg Acc: 0.9493\n",
            "0829 07:10:41 AM Epoch[014/050], Step[0060/0079], Avg Loss: 0.0028, Avg Acc: 0.9506\n",
            "0829 07:10:43 AM ----- Epoch[014/050], Train Loss: 0.0028, Train Acc: 0.9504, time: 8.61\n",
            "0829 07:10:43 AM Now training epoch 16. LR=0.000100\n",
            "0829 07:10:43 AM Epoch[015/050], Step[0000/0079], Avg Loss: 0.0021, Avg Acc: 0.9062\n",
            "0829 07:10:46 AM Epoch[015/050], Step[0020/0079], Avg Loss: 0.0019, Avg Acc: 0.9576\n",
            "0829 07:10:48 AM Epoch[015/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9569\n",
            "0829 07:10:50 AM Epoch[015/050], Step[0060/0079], Avg Loss: 0.0021, Avg Acc: 0.9588\n",
            "0829 07:10:52 AM ----- Epoch[015/050], Train Loss: 0.0024, Train Acc: 0.9574, time: 8.60\n",
            "0829 07:10:52 AM Now training epoch 17. LR=0.000100\n",
            "0829 07:10:52 AM Epoch[016/050], Step[0000/0079], Avg Loss: 0.0021, Avg Acc: 0.9688\n",
            "0829 07:10:54 AM Epoch[016/050], Step[0020/0079], Avg Loss: 0.0041, Avg Acc: 0.9152\n",
            "0829 07:10:56 AM Epoch[016/050], Step[0040/0079], Avg Loss: 0.0039, Avg Acc: 0.9295\n",
            "0829 07:10:59 AM Epoch[016/050], Step[0060/0079], Avg Loss: 0.0036, Avg Acc: 0.9337\n",
            "0829 07:11:00 AM ----- Epoch[016/050], Train Loss: 0.0034, Train Acc: 0.9394, time: 8.60\n",
            "0829 07:11:00 AM Now training epoch 18. LR=0.000100\n",
            "0829 07:11:01 AM Epoch[017/050], Step[0000/0079], Avg Loss: 0.0010, Avg Acc: 0.9688\n",
            "0829 07:11:03 AM Epoch[017/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9613\n",
            "0829 07:11:05 AM Epoch[017/050], Step[0040/0079], Avg Loss: 0.0021, Avg Acc: 0.9619\n",
            "0829 07:11:07 AM Epoch[017/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9631\n",
            "0829 07:11:09 AM ----- Epoch[017/050], Train Loss: 0.0021, Train Acc: 0.9632, time: 8.58\n",
            "0829 07:11:09 AM Now training epoch 19. LR=0.000100\n",
            "0829 07:11:09 AM Epoch[018/050], Step[0000/0079], Avg Loss: 0.0032, Avg Acc: 0.9531\n",
            "0829 07:11:11 AM Epoch[018/050], Step[0020/0079], Avg Loss: 0.0032, Avg Acc: 0.9449\n",
            "0829 07:11:14 AM Epoch[018/050], Step[0040/0079], Avg Loss: 0.0027, Avg Acc: 0.9505\n",
            "0829 07:11:16 AM Epoch[018/050], Step[0060/0079], Avg Loss: 0.0026, Avg Acc: 0.9536\n",
            "0829 07:11:18 AM ----- Epoch[018/050], Train Loss: 0.0025, Train Acc: 0.9548, time: 8.56\n",
            "0829 07:11:18 AM Now training epoch 20. LR=0.000100\n",
            "0829 07:11:18 AM Epoch[019/050], Step[0000/0079], Avg Loss: 0.0046, Avg Acc: 0.9219\n",
            "0829 07:11:20 AM Epoch[019/050], Step[0020/0079], Avg Loss: 0.0021, Avg Acc: 0.9650\n",
            "0829 07:11:22 AM Epoch[019/050], Step[0040/0079], Avg Loss: 0.0026, Avg Acc: 0.9592\n",
            "0829 07:11:24 AM Epoch[019/050], Step[0060/0079], Avg Loss: 0.0025, Avg Acc: 0.9580\n",
            "0829 07:11:26 AM ----- Epoch[019/050], Train Loss: 0.0025, Train Acc: 0.9590, time: 8.61\n",
            "0829 07:11:26 AM Now training epoch 21. LR=0.000100\n",
            "0829 07:11:26 AM Epoch[020/050], Step[0000/0079], Avg Loss: 0.0024, Avg Acc: 0.9531\n",
            "0829 07:11:29 AM Epoch[020/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9546\n",
            "0829 07:11:31 AM Epoch[020/050], Step[0040/0079], Avg Loss: 0.0022, Avg Acc: 0.9607\n",
            "0829 07:11:33 AM Epoch[020/050], Step[0060/0079], Avg Loss: 0.0022, Avg Acc: 0.9618\n",
            "0829 07:11:35 AM ----- Epoch[020/050], Train Loss: 0.0021, Train Acc: 0.9618, time: 8.70\n",
            "0829 07:11:35 AM Now training epoch 22. LR=0.000100\n",
            "0829 07:11:35 AM Epoch[021/050], Step[0000/0079], Avg Loss: 0.0003, Avg Acc: 1.0000\n",
            "0829 07:11:37 AM Epoch[021/050], Step[0020/0079], Avg Loss: 0.0017, Avg Acc: 0.9635\n",
            "0829 07:11:39 AM Epoch[021/050], Step[0040/0079], Avg Loss: 0.0016, Avg Acc: 0.9649\n",
            "0829 07:11:42 AM Epoch[021/050], Step[0060/0079], Avg Loss: 0.0017, Avg Acc: 0.9652\n",
            "0829 07:11:44 AM ----- Epoch[021/050], Train Loss: 0.0016, Train Acc: 0.9662, time: 8.61\n",
            "0829 07:11:44 AM Now training epoch 23. LR=0.000100\n",
            "0829 07:11:44 AM Epoch[022/050], Step[0000/0079], Avg Loss: 0.0020, Avg Acc: 0.9531\n",
            "0829 07:11:46 AM Epoch[022/050], Step[0020/0079], Avg Loss: 0.0016, Avg Acc: 0.9665\n",
            "0829 07:11:48 AM Epoch[022/050], Step[0040/0079], Avg Loss: 0.0015, Avg Acc: 0.9684\n",
            "0829 07:11:50 AM Epoch[022/050], Step[0060/0079], Avg Loss: 0.0016, Avg Acc: 0.9682\n",
            "0829 07:11:52 AM ----- Epoch[022/050], Train Loss: 0.0016, Train Acc: 0.9694, time: 8.61\n",
            "0829 07:11:52 AM Now training epoch 24. LR=0.000100\n",
            "0829 07:11:52 AM Epoch[023/050], Step[0000/0079], Avg Loss: 0.0008, Avg Acc: 0.9688\n",
            "0829 07:11:55 AM Epoch[023/050], Step[0020/0079], Avg Loss: 0.0020, Avg Acc: 0.9591\n",
            "0829 07:11:57 AM Epoch[023/050], Step[0040/0079], Avg Loss: 0.0019, Avg Acc: 0.9657\n",
            "0829 07:11:59 AM Epoch[023/050], Step[0060/0079], Avg Loss: 0.0020, Avg Acc: 0.9652\n",
            "0829 07:12:01 AM ----- Epoch[023/050], Train Loss: 0.0020, Train Acc: 0.9652, time: 8.62\n",
            "0829 07:12:01 AM Now training epoch 25. LR=0.000100\n",
            "0829 07:12:01 AM Epoch[024/050], Step[0000/0079], Avg Loss: 0.0004, Avg Acc: 1.0000\n",
            "0829 07:12:03 AM Epoch[024/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9784\n",
            "0829 07:12:05 AM Epoch[024/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9768\n",
            "0829 07:12:07 AM Epoch[024/050], Step[0060/0079], Avg Loss: 0.0016, Avg Acc: 0.9716\n",
            "0829 07:12:09 AM ----- Epoch[024/050], Train Loss: 0.0015, Train Acc: 0.9716, time: 8.59\n",
            "0829 07:12:09 AM Now training epoch 26. LR=0.000100\n",
            "0829 07:12:10 AM Epoch[025/050], Step[0000/0079], Avg Loss: 0.0009, Avg Acc: 0.9844\n",
            "0829 07:12:12 AM Epoch[025/050], Step[0020/0079], Avg Loss: 0.0014, Avg Acc: 0.9740\n",
            "0829 07:12:14 AM Epoch[025/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9756\n",
            "0829 07:12:16 AM Epoch[025/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9757\n",
            "0829 07:12:18 AM ----- Epoch[025/050], Train Loss: 0.0016, Train Acc: 0.9748, time: 8.62\n",
            "0829 07:12:22 AM ----- Save model: ./outputs/train-20220829-07-08-40/Epoch-25-Loss-0.0015669643804430962.pth\n",
            "0829 07:12:22 AM Now training epoch 27. LR=0.000100\n",
            "0829 07:12:23 AM Epoch[026/050], Step[0000/0079], Avg Loss: 0.0006, Avg Acc: 0.9844\n",
            "0829 07:12:25 AM Epoch[026/050], Step[0020/0079], Avg Loss: 0.0016, Avg Acc: 0.9717\n",
            "0829 07:12:27 AM Epoch[026/050], Step[0040/0079], Avg Loss: 0.0015, Avg Acc: 0.9726\n",
            "0829 07:12:29 AM Epoch[026/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9749\n",
            "0829 07:12:31 AM ----- Epoch[026/050], Train Loss: 0.0014, Train Acc: 0.9756, time: 8.66\n",
            "0829 07:12:31 AM Now training epoch 28. LR=0.000100\n",
            "0829 07:12:31 AM Epoch[027/050], Step[0000/0079], Avg Loss: 0.0017, Avg Acc: 0.9688\n",
            "0829 07:12:34 AM Epoch[027/050], Step[0020/0079], Avg Loss: 0.0021, Avg Acc: 0.9650\n",
            "0829 07:12:36 AM Epoch[027/050], Step[0040/0079], Avg Loss: 0.0017, Avg Acc: 0.9691\n",
            "0829 07:12:38 AM Epoch[027/050], Step[0060/0079], Avg Loss: 0.0016, Avg Acc: 0.9708\n",
            "0829 07:12:40 AM ----- Epoch[027/050], Train Loss: 0.0017, Train Acc: 0.9698, time: 8.64\n",
            "0829 07:12:40 AM Now training epoch 29. LR=0.000100\n",
            "0829 07:12:40 AM Epoch[028/050], Step[0000/0079], Avg Loss: 0.0008, Avg Acc: 0.9688\n",
            "0829 07:12:42 AM Epoch[028/050], Step[0020/0079], Avg Loss: 0.0020, Avg Acc: 0.9643\n",
            "0829 07:12:44 AM Epoch[028/050], Step[0040/0079], Avg Loss: 0.0019, Avg Acc: 0.9653\n",
            "0829 07:12:46 AM Epoch[028/050], Step[0060/0079], Avg Loss: 0.0017, Avg Acc: 0.9685\n",
            "0829 07:12:48 AM ----- Epoch[028/050], Train Loss: 0.0017, Train Acc: 0.9692, time: 8.62\n",
            "0829 07:12:48 AM Now training epoch 30. LR=0.000100\n",
            "0829 07:12:49 AM Epoch[029/050], Step[0000/0079], Avg Loss: 0.0012, Avg Acc: 0.9688\n",
            "0829 07:12:51 AM Epoch[029/050], Step[0020/0079], Avg Loss: 0.0013, Avg Acc: 0.9762\n",
            "0829 07:12:53 AM Epoch[029/050], Step[0040/0079], Avg Loss: 0.0015, Avg Acc: 0.9737\n",
            "0829 07:12:55 AM Epoch[029/050], Step[0060/0079], Avg Loss: 0.0016, Avg Acc: 0.9728\n",
            "0829 07:12:57 AM ----- Epoch[029/050], Train Loss: 0.0017, Train Acc: 0.9718, time: 8.65\n",
            "0829 07:12:57 AM Now training epoch 31. LR=0.000100\n",
            "0829 07:12:57 AM Epoch[030/050], Step[0000/0079], Avg Loss: 0.0020, Avg Acc: 0.9531\n",
            "0829 07:12:59 AM Epoch[030/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9650\n",
            "0829 07:13:02 AM Epoch[030/050], Step[0040/0079], Avg Loss: 0.0020, Avg Acc: 0.9699\n",
            "0829 07:13:04 AM Epoch[030/050], Step[0060/0079], Avg Loss: 0.0018, Avg Acc: 0.9723\n",
            "0829 07:13:06 AM ----- Epoch[030/050], Train Loss: 0.0017, Train Acc: 0.9738, time: 8.61\n",
            "0829 07:13:06 AM Now training epoch 32. LR=0.000100\n",
            "0829 07:13:06 AM Epoch[031/050], Step[0000/0079], Avg Loss: 0.0054, Avg Acc: 0.8750\n",
            "0829 07:13:08 AM Epoch[031/050], Step[0020/0079], Avg Loss: 0.0023, Avg Acc: 0.9568\n",
            "0829 07:13:10 AM Epoch[031/050], Step[0040/0079], Avg Loss: 0.0018, Avg Acc: 0.9657\n",
            "0829 07:13:12 AM Epoch[031/050], Step[0060/0079], Avg Loss: 0.0016, Avg Acc: 0.9716\n",
            "0829 07:13:14 AM ----- Epoch[031/050], Train Loss: 0.0015, Train Acc: 0.9740, time: 8.63\n",
            "0829 07:13:14 AM Now training epoch 33. LR=0.000100\n",
            "0829 07:13:15 AM Epoch[032/050], Step[0000/0079], Avg Loss: 0.0035, Avg Acc: 0.9688\n",
            "0829 07:13:17 AM Epoch[032/050], Step[0020/0079], Avg Loss: 0.0020, Avg Acc: 0.9702\n",
            "0829 07:13:19 AM Epoch[032/050], Step[0040/0079], Avg Loss: 0.0023, Avg Acc: 0.9649\n",
            "0829 07:13:21 AM Epoch[032/050], Step[0060/0079], Avg Loss: 0.0019, Avg Acc: 0.9708\n",
            "0829 07:13:23 AM ----- Epoch[032/050], Train Loss: 0.0018, Train Acc: 0.9724, time: 8.61\n",
            "0829 07:13:23 AM Now training epoch 34. LR=0.000100\n",
            "0829 07:13:23 AM Epoch[033/050], Step[0000/0079], Avg Loss: 0.0017, Avg Acc: 0.9531\n",
            "0829 07:13:25 AM Epoch[033/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9814\n",
            "0829 07:13:27 AM Epoch[033/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9771\n",
            "0829 07:13:30 AM Epoch[033/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9767\n",
            "0829 07:13:32 AM ----- Epoch[033/050], Train Loss: 0.0013, Train Acc: 0.9758, time: 8.63\n",
            "0829 07:13:32 AM Now training epoch 35. LR=0.000100\n",
            "0829 07:13:32 AM Epoch[034/050], Step[0000/0079], Avg Loss: 0.0006, Avg Acc: 0.9844\n",
            "0829 07:13:34 AM Epoch[034/050], Step[0020/0079], Avg Loss: 0.0012, Avg Acc: 0.9799\n",
            "0829 07:13:36 AM Epoch[034/050], Step[0040/0079], Avg Loss: 0.0010, Avg Acc: 0.9825\n",
            "0829 07:13:38 AM Epoch[034/050], Step[0060/0079], Avg Loss: 0.0010, Avg Acc: 0.9836\n",
            "0829 07:13:40 AM ----- Epoch[034/050], Train Loss: 0.0011, Train Acc: 0.9824, time: 8.59\n",
            "0829 07:13:40 AM Now training epoch 36. LR=0.000100\n",
            "0829 07:13:40 AM Epoch[035/050], Step[0000/0079], Avg Loss: 0.0008, Avg Acc: 0.9844\n",
            "0829 07:13:42 AM Epoch[035/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9807\n",
            "0829 07:13:45 AM Epoch[035/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9817\n",
            "0829 07:13:47 AM Epoch[035/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9803\n",
            "0829 07:13:49 AM ----- Epoch[035/050], Train Loss: 0.0012, Train Acc: 0.9796, time: 8.73\n",
            "0829 07:13:49 AM Now training epoch 37. LR=0.000100\n",
            "0829 07:13:49 AM Epoch[036/050], Step[0000/0079], Avg Loss: 0.0024, Avg Acc: 0.9375\n",
            "0829 07:13:51 AM Epoch[036/050], Step[0020/0079], Avg Loss: 0.0013, Avg Acc: 0.9792\n",
            "0829 07:13:53 AM Epoch[036/050], Step[0040/0079], Avg Loss: 0.0015, Avg Acc: 0.9790\n",
            "0829 07:13:56 AM Epoch[036/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9808\n",
            "0829 07:13:57 AM ----- Epoch[036/050], Train Loss: 0.0013, Train Acc: 0.9824, time: 8.62\n",
            "0829 07:13:57 AM Now training epoch 38. LR=0.000100\n",
            "0829 07:13:58 AM Epoch[037/050], Step[0000/0079], Avg Loss: 0.0024, Avg Acc: 0.9531\n",
            "0829 07:14:00 AM Epoch[037/050], Step[0020/0079], Avg Loss: 0.0014, Avg Acc: 0.9747\n",
            "0829 07:14:02 AM Epoch[037/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9760\n",
            "0829 07:14:04 AM Epoch[037/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9762\n",
            "0829 07:14:06 AM ----- Epoch[037/050], Train Loss: 0.0015, Train Acc: 0.9758, time: 8.64\n",
            "0829 07:14:06 AM Now training epoch 39. LR=0.000100\n",
            "0829 07:14:06 AM Epoch[038/050], Step[0000/0079], Avg Loss: 0.0011, Avg Acc: 0.9844\n",
            "0829 07:14:08 AM Epoch[038/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9866\n",
            "0829 07:14:11 AM Epoch[038/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9825\n",
            "0829 07:14:13 AM Epoch[038/050], Step[0060/0079], Avg Loss: 0.0012, Avg Acc: 0.9813\n",
            "0829 07:14:15 AM ----- Epoch[038/050], Train Loss: 0.0011, Train Acc: 0.9820, time: 8.62\n",
            "0829 07:14:15 AM Now training epoch 40. LR=0.000100\n",
            "0829 07:14:15 AM Epoch[039/050], Step[0000/0079], Avg Loss: 0.0010, Avg Acc: 0.9844\n",
            "0829 07:14:17 AM Epoch[039/050], Step[0020/0079], Avg Loss: 0.0016, Avg Acc: 0.9769\n",
            "0829 07:14:19 AM Epoch[039/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9809\n",
            "0829 07:14:21 AM Epoch[039/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9823\n",
            "0829 07:14:23 AM ----- Epoch[039/050], Train Loss: 0.0016, Train Acc: 0.9766, time: 8.59\n",
            "0829 07:14:23 AM Now training epoch 41. LR=0.000100\n",
            "0829 07:14:24 AM Epoch[040/050], Step[0000/0079], Avg Loss: 0.0008, Avg Acc: 1.0000\n",
            "0829 07:14:26 AM Epoch[040/050], Step[0020/0079], Avg Loss: 0.0024, Avg Acc: 0.9621\n",
            "0829 07:14:28 AM Epoch[040/050], Step[0040/0079], Avg Loss: 0.0019, Avg Acc: 0.9672\n",
            "0829 07:14:30 AM Epoch[040/050], Step[0060/0079], Avg Loss: 0.0016, Avg Acc: 0.9728\n",
            "0829 07:14:32 AM ----- Epoch[040/050], Train Loss: 0.0014, Train Acc: 0.9762, time: 8.60\n",
            "0829 07:14:32 AM Now training epoch 42. LR=0.000100\n",
            "0829 07:14:32 AM Epoch[041/050], Step[0000/0079], Avg Loss: 0.0016, Avg Acc: 0.9844\n",
            "0829 07:14:34 AM Epoch[041/050], Step[0020/0079], Avg Loss: 0.0006, Avg Acc: 0.9874\n",
            "0829 07:14:36 AM Epoch[041/050], Step[0040/0079], Avg Loss: 0.0010, Avg Acc: 0.9813\n",
            "0829 07:14:39 AM Epoch[041/050], Step[0060/0079], Avg Loss: 0.0010, Avg Acc: 0.9800\n",
            "0829 07:14:41 AM ----- Epoch[041/050], Train Loss: 0.0010, Train Acc: 0.9792, time: 8.63\n",
            "0829 07:14:41 AM Now training epoch 43. LR=0.000100\n",
            "0829 07:14:41 AM Epoch[042/050], Step[0000/0079], Avg Loss: 0.0014, Avg Acc: 0.9688\n",
            "0829 07:14:43 AM Epoch[042/050], Step[0020/0079], Avg Loss: 0.0010, Avg Acc: 0.9814\n",
            "0829 07:14:45 AM Epoch[042/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9768\n",
            "0829 07:14:47 AM Epoch[042/050], Step[0060/0079], Avg Loss: 0.0013, Avg Acc: 0.9749\n",
            "0829 07:14:49 AM ----- Epoch[042/050], Train Loss: 0.0014, Train Acc: 0.9732, time: 8.60\n",
            "0829 07:14:49 AM Now training epoch 44. LR=0.000100\n",
            "0829 07:14:49 AM Epoch[043/050], Step[0000/0079], Avg Loss: 0.0009, Avg Acc: 0.9844\n",
            "0829 07:14:52 AM Epoch[043/050], Step[0020/0079], Avg Loss: 0.0022, Avg Acc: 0.9658\n",
            "0829 07:14:54 AM Epoch[043/050], Step[0040/0079], Avg Loss: 0.0018, Avg Acc: 0.9722\n",
            "0829 07:14:56 AM Epoch[043/050], Step[0060/0079], Avg Loss: 0.0015, Avg Acc: 0.9769\n",
            "0829 07:14:58 AM ----- Epoch[043/050], Train Loss: 0.0014, Train Acc: 0.9786, time: 8.61\n",
            "0829 07:14:58 AM Now training epoch 45. LR=0.000100\n",
            "0829 07:14:58 AM Epoch[044/050], Step[0000/0079], Avg Loss: 0.0005, Avg Acc: 1.0000\n",
            "0829 07:15:00 AM Epoch[044/050], Step[0020/0079], Avg Loss: 0.0006, Avg Acc: 0.9896\n",
            "0829 07:15:02 AM Epoch[044/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9825\n",
            "0829 07:15:04 AM Epoch[044/050], Step[0060/0079], Avg Loss: 0.0015, Avg Acc: 0.9782\n",
            "0829 07:15:06 AM ----- Epoch[044/050], Train Loss: 0.0014, Train Acc: 0.9788, time: 8.59\n",
            "0829 07:15:06 AM Now training epoch 46. LR=0.000100\n",
            "0829 07:15:07 AM Epoch[045/050], Step[0000/0079], Avg Loss: 0.0011, Avg Acc: 0.9688\n",
            "0829 07:15:09 AM Epoch[045/050], Step[0020/0079], Avg Loss: 0.0013, Avg Acc: 0.9754\n",
            "0829 07:15:11 AM Epoch[045/050], Step[0040/0079], Avg Loss: 0.0011, Avg Acc: 0.9806\n",
            "0829 07:15:13 AM Epoch[045/050], Step[0060/0079], Avg Loss: 0.0011, Avg Acc: 0.9818\n",
            "0829 07:15:15 AM ----- Epoch[045/050], Train Loss: 0.0011, Train Acc: 0.9812, time: 8.60\n",
            "0829 07:15:15 AM Now training epoch 47. LR=0.000100\n",
            "0829 07:15:15 AM Epoch[046/050], Step[0000/0079], Avg Loss: 0.0004, Avg Acc: 1.0000\n",
            "0829 07:15:17 AM Epoch[046/050], Step[0020/0079], Avg Loss: 0.0009, Avg Acc: 0.9821\n",
            "0829 07:15:19 AM Epoch[046/050], Step[0040/0079], Avg Loss: 0.0007, Avg Acc: 0.9867\n",
            "0829 07:15:22 AM Epoch[046/050], Step[0060/0079], Avg Loss: 0.0006, Avg Acc: 0.9882\n",
            "0829 07:15:24 AM ----- Epoch[046/050], Train Loss: 0.0006, Train Acc: 0.9878, time: 8.61\n",
            "0829 07:15:24 AM Now training epoch 48. LR=0.000100\n",
            "0829 07:15:24 AM Epoch[047/050], Step[0000/0079], Avg Loss: 0.0000, Avg Acc: 1.0000\n",
            "0829 07:15:26 AM Epoch[047/050], Step[0020/0079], Avg Loss: 0.0004, Avg Acc: 0.9918\n",
            "0829 07:15:28 AM Epoch[047/050], Step[0040/0079], Avg Loss: 0.0005, Avg Acc: 0.9905\n",
            "0829 07:15:30 AM Epoch[047/050], Step[0060/0079], Avg Loss: 0.0007, Avg Acc: 0.9882\n",
            "0829 07:15:32 AM ----- Epoch[047/050], Train Loss: 0.0008, Train Acc: 0.9874, time: 8.64\n",
            "0829 07:15:32 AM Now training epoch 49. LR=0.000100\n",
            "0829 07:15:32 AM Epoch[048/050], Step[0000/0079], Avg Loss: 0.0001, Avg Acc: 1.0000\n",
            "0829 07:15:35 AM Epoch[048/050], Step[0020/0079], Avg Loss: 0.0008, Avg Acc: 0.9859\n",
            "0829 07:15:37 AM Epoch[048/050], Step[0040/0079], Avg Loss: 0.0008, Avg Acc: 0.9863\n",
            "0829 07:15:39 AM Epoch[048/050], Step[0060/0079], Avg Loss: 0.0008, Avg Acc: 0.9857\n",
            "0829 07:15:41 AM ----- Epoch[048/050], Train Loss: 0.0008, Train Acc: 0.9862, time: 8.62\n",
            "0829 07:15:41 AM Now training epoch 50. LR=0.000100\n",
            "0829 07:15:41 AM Epoch[049/050], Step[0000/0079], Avg Loss: 0.0005, Avg Acc: 1.0000\n",
            "0829 07:15:43 AM Epoch[049/050], Step[0020/0079], Avg Loss: 0.0011, Avg Acc: 0.9821\n",
            "0829 07:15:45 AM Epoch[049/050], Step[0040/0079], Avg Loss: 0.0012, Avg Acc: 0.9794\n",
            "0829 07:15:48 AM Epoch[049/050], Step[0060/0079], Avg Loss: 0.0010, Avg Acc: 0.9828\n",
            "0829 07:15:49 AM ----- Epoch[049/050], Train Loss: 0.0009, Train Acc: 0.9854, time: 8.61\n",
            "0829 07:15:49 AM Now training epoch 51. LR=0.000100\n",
            "0829 07:15:50 AM Epoch[050/050], Step[0000/0079], Avg Loss: 0.0000, Avg Acc: 1.0000\n",
            "0829 07:15:52 AM Epoch[050/050], Step[0020/0079], Avg Loss: 0.0017, Avg Acc: 0.9799\n",
            "0829 07:15:54 AM Epoch[050/050], Step[0040/0079], Avg Loss: 0.0013, Avg Acc: 0.9809\n",
            "0829 07:15:56 AM Epoch[050/050], Step[0060/0079], Avg Loss: 0.0014, Avg Acc: 0.9795\n",
            "0829 07:15:58 AM ----- Epoch[050/050], Train Loss: 0.0012, Train Acc: 0.9810, time: 8.59\n",
            "0829 07:16:02 AM ----- Save model: ./outputs/train-20220829-07-08-40/Epoch-50-Loss-0.001244762172200717.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXWg0B8rroIa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}